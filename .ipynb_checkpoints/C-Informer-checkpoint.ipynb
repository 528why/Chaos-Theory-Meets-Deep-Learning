{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Informer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaIHYx4_ECL"
   },
   "source": [
    "## Download code and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SA_i2gbl-rn-",
    "outputId": "41dadbf7-6be2-423c-ed51-1e321f42be68"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/zhouhaoyi/Informer2020.git\n",
    "# !git clone https://github.com/zhouhaoyi/ETDataset.git\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# if not 'Informer2020' in sys.path:\n",
    "#     sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YW9TS6jp_YXc"
   },
   "outputs": [],
   "source": [
    "# !pip install -r ./Informer2020/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe"
   },
   "source": [
    "## Experiments: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RPdt-Kwc_RRZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "import time\n",
    "from torchvision import models\n",
    "from gpu_mem_track import MemTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6mx2dnwY9dWi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'ECL' # data\n",
    "args.root_path = './data/ETT/' # root path of data file\n",
    "args.data_path = 'ECL.csv' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'MT_320' # target feature in S or MS task\n",
    "args.freq = 'd' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 48 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 321 # encoder input size\n",
    "args.dec_in = 321 # decoder input size\n",
    "args.c_out = 10 # output size\n",
    "args.factor = 3 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr =1\n",
    "args.train_epochs = 100\n",
    "args.patience = 100\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "\n",
    "#args.inverse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k_BCYODAwKl9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "53o3pZ809p-a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yZ5Q2vyKwSfk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywY-umrw-mHO",
    "outputId": "9b2e2e8e-7025-46e6-e2d7-2d8c9ceeb381",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'WTH', 'root_path': './data/ETT/', 'data_path': 'WTH.csv', 'features': 'MS', 'target': 'WetBulbCelsius', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 48, 'label_len': 48, 'pred_len': 24, 'enc_in': 12, 'dec_in': 12, 'c_out': 10, 'factor': 3, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 100, 'patience': 100, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KVHZhRB4-on9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "928tzaA2AA2g",
    "outputId": "c19f673a-02d1-4f4d-91c3-d0f25e600443",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序开始时间: 2024-05-17 12:17:01\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_WTH_ftMS_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc3_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 24473\n",
      "val 3485\n",
      "test 6989\n",
      "\titers: 100, epoch: 1 | loss: 0.3415342\n",
      "\tspeed: 0.0876s/iter; left time: 6682.9530s\n",
      "\titers: 200, epoch: 1 | loss: 0.2569611\n",
      "\tspeed: 0.0807s/iter; left time: 6146.7456s\n",
      "\titers: 300, epoch: 1 | loss: 0.1374667\n",
      "\tspeed: 0.0813s/iter; left time: 6189.4412s\n",
      "\titers: 400, epoch: 1 | loss: 0.2233350\n",
      "\tspeed: 0.0809s/iter; left time: 6145.0690s\n",
      "\titers: 500, epoch: 1 | loss: 0.2058273\n",
      "\tspeed: 0.0804s/iter; left time: 6102.8132s\n",
      "\titers: 600, epoch: 1 | loss: 0.1650656\n",
      "\tspeed: 0.0801s/iter; left time: 6074.3946s\n",
      "\titers: 700, epoch: 1 | loss: 0.1606403\n",
      "\tspeed: 0.0804s/iter; left time: 6087.2233s\n",
      "Epoch: 1 cost time: 62.255565881729126\n",
      "Epoch: 1, Steps: 764 | Train Loss: 0.1956349 Vali Loss: 0.2403265 Test Loss: 0.1486002\n",
      "Validation loss decreased (inf --> 0.240326).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1417729\n",
      "\tspeed: 0.2264s/iter; left time: 17100.5185s\n",
      "\titers: 200, epoch: 2 | loss: 0.1194989\n",
      "\tspeed: 0.0801s/iter; left time: 6041.9776s\n",
      "\titers: 300, epoch: 2 | loss: 0.1566957\n",
      "\tspeed: 0.0800s/iter; left time: 6025.6969s\n",
      "\titers: 400, epoch: 2 | loss: 0.1503654\n",
      "\tspeed: 0.0805s/iter; left time: 6060.0048s\n",
      "\titers: 500, epoch: 2 | loss: 0.1109111\n",
      "\tspeed: 0.0804s/iter; left time: 6042.8873s\n",
      "\titers: 600, epoch: 2 | loss: 0.1618888\n",
      "\tspeed: 0.0798s/iter; left time: 5987.0588s\n",
      "\titers: 700, epoch: 2 | loss: 0.1141719\n",
      "\tspeed: 0.0795s/iter; left time: 5959.9664s\n",
      "Epoch: 2 cost time: 60.19661521911621\n",
      "Epoch: 2, Steps: 764 | Train Loss: 0.1460470 Vali Loss: 0.2045455 Test Loss: 0.1749173\n",
      "Validation loss decreased (0.240326 --> 0.204545).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1461481\n",
      "\tspeed: 0.2240s/iter; left time: 16750.4649s\n",
      "\titers: 200, epoch: 3 | loss: 0.1475942\n",
      "\tspeed: 0.0807s/iter; left time: 6029.3162s\n",
      "\titers: 300, epoch: 3 | loss: 0.1502576\n",
      "\tspeed: 0.0808s/iter; left time: 6026.2702s\n",
      "\titers: 400, epoch: 3 | loss: 0.1568356\n",
      "\tspeed: 0.0811s/iter; left time: 6043.3419s\n",
      "\titers: 500, epoch: 3 | loss: 0.1049397\n",
      "\tspeed: 0.0808s/iter; left time: 6007.3472s\n",
      "\titers: 600, epoch: 3 | loss: 0.1124449\n",
      "\tspeed: 0.0804s/iter; left time: 5973.6283s\n",
      "\titers: 700, epoch: 3 | loss: 0.1194088\n",
      "\tspeed: 0.0800s/iter; left time: 5931.6542s\n",
      "Epoch: 3 cost time: 60.80476975440979\n",
      "Epoch: 3, Steps: 764 | Train Loss: 0.1253123 Vali Loss: 0.2138048 Test Loss: 0.1236476\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1195937\n",
      "\tspeed: 0.2187s/iter; left time: 16187.5924s\n",
      "\titers: 200, epoch: 4 | loss: 0.1357097\n",
      "\tspeed: 0.0807s/iter; left time: 5963.2871s\n",
      "\titers: 300, epoch: 4 | loss: 0.1178964\n",
      "\tspeed: 0.0809s/iter; left time: 5972.3953s\n",
      "\titers: 400, epoch: 4 | loss: 0.0828979\n",
      "\tspeed: 0.0812s/iter; left time: 5981.8804s\n",
      "\titers: 500, epoch: 4 | loss: 0.0939050\n",
      "\tspeed: 0.0810s/iter; left time: 5962.9698s\n",
      "\titers: 600, epoch: 4 | loss: 0.1249857\n",
      "\tspeed: 0.0810s/iter; left time: 5956.4503s\n",
      "\titers: 700, epoch: 4 | loss: 0.0858125\n",
      "\tspeed: 0.0807s/iter; left time: 5924.2088s\n",
      "Epoch: 4 cost time: 61.42904257774353\n",
      "Epoch: 4, Steps: 764 | Train Loss: 0.1150665 Vali Loss: 0.1991879 Test Loss: 0.1229908\n",
      "Validation loss decreased (0.204545 --> 0.199188).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1058423\n",
      "\tspeed: 0.2258s/iter; left time: 16538.6651s\n",
      "\titers: 200, epoch: 5 | loss: 0.0946447\n",
      "\tspeed: 0.0810s/iter; left time: 5928.0510s\n",
      "\titers: 300, epoch: 5 | loss: 0.0953854\n",
      "\tspeed: 0.0809s/iter; left time: 5905.9784s\n",
      "\titers: 400, epoch: 5 | loss: 0.0843530\n",
      "\tspeed: 0.0808s/iter; left time: 5897.0247s\n",
      "\titers: 500, epoch: 5 | loss: 0.0922337\n",
      "\tspeed: 0.0807s/iter; left time: 5880.5326s\n",
      "\titers: 600, epoch: 5 | loss: 0.1124832\n",
      "\tspeed: 0.0799s/iter; left time: 5813.4333s\n",
      "\titers: 700, epoch: 5 | loss: 0.1015847\n",
      "\tspeed: 0.0814s/iter; left time: 5909.9906s\n",
      "Epoch: 5 cost time: 61.24783682823181\n",
      "Epoch: 5, Steps: 764 | Train Loss: 0.1093576 Vali Loss: 0.1815657 Test Loss: 0.1083001\n",
      "Validation loss decreased (0.199188 --> 0.181566).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0985480\n",
      "\tspeed: 0.2227s/iter; left time: 16139.6997s\n",
      "\titers: 200, epoch: 6 | loss: 0.0734685\n",
      "\tspeed: 0.0805s/iter; left time: 5826.7757s\n",
      "\titers: 300, epoch: 6 | loss: 0.0960166\n",
      "\tspeed: 0.0798s/iter; left time: 5768.3840s\n",
      "\titers: 400, epoch: 6 | loss: 0.1013178\n",
      "\tspeed: 0.0800s/iter; left time: 5772.5916s\n",
      "\titers: 500, epoch: 6 | loss: 0.1015482\n",
      "\tspeed: 0.0797s/iter; left time: 5741.8586s\n",
      "\titers: 600, epoch: 6 | loss: 0.0771411\n",
      "\tspeed: 0.0810s/iter; left time: 5827.1424s\n",
      "\titers: 700, epoch: 6 | loss: 0.1233465\n",
      "\tspeed: 0.0788s/iter; left time: 5662.3300s\n",
      "Epoch: 6 cost time: 60.42560362815857\n",
      "Epoch: 6, Steps: 764 | Train Loss: 0.1060193 Vali Loss: 0.1794370 Test Loss: 0.1086499\n",
      "Validation loss decreased (0.181566 --> 0.179437).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0838223\n",
      "\tspeed: 0.2193s/iter; left time: 15731.0376s\n",
      "\titers: 200, epoch: 7 | loss: 0.0872467\n",
      "\tspeed: 0.0801s/iter; left time: 5737.8556s\n",
      "\titers: 300, epoch: 7 | loss: 0.1222271\n",
      "\tspeed: 0.0807s/iter; left time: 5771.3565s\n",
      "\titers: 400, epoch: 7 | loss: 0.1015873\n",
      "\tspeed: 0.0804s/iter; left time: 5744.9407s\n",
      "\titers: 500, epoch: 7 | loss: 0.1037520\n",
      "\tspeed: 0.0800s/iter; left time: 5705.5052s\n",
      "\titers: 600, epoch: 7 | loss: 0.0647963\n",
      "\tspeed: 0.0810s/iter; left time: 5769.5237s\n",
      "\titers: 700, epoch: 7 | loss: 0.0866139\n",
      "\tspeed: 0.0788s/iter; left time: 5604.4936s\n",
      "Epoch: 7 cost time: 60.697514295578\n",
      "Epoch: 7, Steps: 764 | Train Loss: 0.1042466 Vali Loss: 0.1791079 Test Loss: 0.1028545\n",
      "Validation loss decreased (0.179437 --> 0.179108).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1203715\n",
      "\tspeed: 0.2308s/iter; left time: 16374.4375s\n",
      "\titers: 200, epoch: 8 | loss: 0.0997552\n",
      "\tspeed: 0.0798s/iter; left time: 5656.1967s\n",
      "\titers: 300, epoch: 8 | loss: 0.1120955\n",
      "\tspeed: 0.0807s/iter; left time: 5712.6148s\n",
      "\titers: 400, epoch: 8 | loss: 0.0784951\n",
      "\tspeed: 0.0803s/iter; left time: 5673.0178s\n",
      "\titers: 500, epoch: 8 | loss: 0.1208990\n",
      "\tspeed: 0.0802s/iter; left time: 5660.2886s\n",
      "\titers: 600, epoch: 8 | loss: 0.1088662\n",
      "\tspeed: 0.0801s/iter; left time: 5641.9175s\n",
      "\titers: 700, epoch: 8 | loss: 0.1251522\n",
      "\tspeed: 0.0746s/iter; left time: 5248.1032s\n",
      "Epoch: 8 cost time: 60.200865745544434\n",
      "Epoch: 8, Steps: 764 | Train Loss: 0.1030933 Vali Loss: 0.1799161 Test Loss: 0.1041641\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1283940\n",
      "\tspeed: 0.2269s/iter; left time: 15927.6282s\n",
      "\titers: 200, epoch: 9 | loss: 0.0949000\n",
      "\tspeed: 0.0807s/iter; left time: 5658.0005s\n",
      "\titers: 300, epoch: 9 | loss: 0.1376795\n",
      "\tspeed: 0.0808s/iter; left time: 5654.4989s\n",
      "\titers: 400, epoch: 9 | loss: 0.1371246\n",
      "\tspeed: 0.0804s/iter; left time: 5617.1234s\n",
      "\titers: 500, epoch: 9 | loss: 0.0929080\n",
      "\tspeed: 0.0807s/iter; left time: 5628.6832s\n",
      "\titers: 600, epoch: 9 | loss: 0.0997319\n",
      "\tspeed: 0.0803s/iter; left time: 5597.7445s\n",
      "\titers: 700, epoch: 9 | loss: 0.0877931\n",
      "\tspeed: 0.0708s/iter; left time: 4927.6338s\n",
      "Epoch: 9 cost time: 59.393794298172\n",
      "Epoch: 9, Steps: 764 | Train Loss: 0.1025574 Vali Loss: 0.1777637 Test Loss: 0.1043408\n",
      "Validation loss decreased (0.179108 --> 0.177764).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1043064\n",
      "\tspeed: 0.2315s/iter; left time: 16069.0010s\n",
      "\titers: 200, epoch: 10 | loss: 0.0746694\n",
      "\tspeed: 0.0803s/iter; left time: 5567.7227s\n",
      "\titers: 300, epoch: 10 | loss: 0.0985783\n",
      "\tspeed: 0.0801s/iter; left time: 5545.0621s\n",
      "\titers: 400, epoch: 10 | loss: 0.0953439\n",
      "\tspeed: 0.0799s/iter; left time: 5526.2561s\n",
      "\titers: 500, epoch: 10 | loss: 0.1127849\n",
      "\tspeed: 0.0812s/iter; left time: 5605.0890s\n",
      "\titers: 600, epoch: 10 | loss: 0.1206907\n",
      "\tspeed: 0.0777s/iter; left time: 5356.8080s\n",
      "\titers: 700, epoch: 10 | loss: 0.1060621\n",
      "\tspeed: 0.0724s/iter; left time: 4981.7003s\n",
      "Epoch: 10 cost time: 60.35854959487915\n",
      "Epoch: 10, Steps: 764 | Train Loss: 0.1025961 Vali Loss: 0.1796831 Test Loss: 0.1035294\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.0895046\n",
      "\tspeed: 0.2320s/iter; left time: 15926.9044s\n",
      "\titers: 200, epoch: 11 | loss: 0.1206401\n",
      "\tspeed: 0.0806s/iter; left time: 5527.2012s\n",
      "\titers: 300, epoch: 11 | loss: 0.0796229\n",
      "\tspeed: 0.0807s/iter; left time: 5525.7139s\n",
      "\titers: 400, epoch: 11 | loss: 0.1272730\n",
      "\tspeed: 0.0813s/iter; left time: 5555.4453s\n",
      "\titers: 500, epoch: 11 | loss: 0.0903167\n",
      "\tspeed: 0.0812s/iter; left time: 5540.7845s\n",
      "\titers: 600, epoch: 11 | loss: 0.1257748\n",
      "\tspeed: 0.0739s/iter; left time: 5038.5805s\n",
      "\titers: 700, epoch: 11 | loss: 0.0869339\n",
      "\tspeed: 0.0737s/iter; left time: 5015.8654s\n",
      "Epoch: 11 cost time: 60.33712553977966\n",
      "Epoch: 11, Steps: 764 | Train Loss: 0.1024313 Vali Loss: 0.1793965 Test Loss: 0.1028223\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.0973040\n",
      "\tspeed: 0.2335s/iter; left time: 15854.6259s\n",
      "\titers: 200, epoch: 12 | loss: 0.0996473\n",
      "\tspeed: 0.0810s/iter; left time: 5491.3279s\n",
      "\titers: 300, epoch: 12 | loss: 0.0827782\n",
      "\tspeed: 0.0812s/iter; left time: 5498.2076s\n",
      "\titers: 400, epoch: 12 | loss: 0.1107216\n",
      "\tspeed: 0.0807s/iter; left time: 5457.1301s\n",
      "\titers: 500, epoch: 12 | loss: 0.1063240\n",
      "\tspeed: 0.0791s/iter; left time: 5336.5212s\n",
      "\titers: 600, epoch: 12 | loss: 0.0875871\n",
      "\tspeed: 0.0714s/iter; left time: 4814.7781s\n",
      "\titers: 700, epoch: 12 | loss: 0.1068739\n",
      "\tspeed: 0.0779s/iter; left time: 5244.5062s\n",
      "Epoch: 12 cost time: 60.35787057876587\n",
      "Epoch: 12, Steps: 764 | Train Loss: 0.1018782 Vali Loss: 0.1797407 Test Loss: 0.1041442\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1277572\n",
      "\tspeed: 0.2334s/iter; left time: 15667.6307s\n",
      "\titers: 200, epoch: 13 | loss: 0.1845681\n",
      "\tspeed: 0.0804s/iter; left time: 5387.5612s\n",
      "\titers: 300, epoch: 13 | loss: 0.1133329\n",
      "\tspeed: 0.0810s/iter; left time: 5424.7902s\n",
      "\titers: 400, epoch: 13 | loss: 0.0948997\n",
      "\tspeed: 0.0804s/iter; left time: 5373.9970s\n",
      "\titers: 500, epoch: 13 | loss: 0.1093419\n",
      "\tspeed: 0.0768s/iter; left time: 5125.4281s\n",
      "\titers: 600, epoch: 13 | loss: 0.0965119\n",
      "\tspeed: 0.0701s/iter; left time: 4672.2901s\n",
      "\titers: 700, epoch: 13 | loss: 0.1332175\n",
      "\tspeed: 0.0806s/iter; left time: 5362.3264s\n",
      "Epoch: 13 cost time: 60.08415746688843\n",
      "Epoch: 13, Steps: 764 | Train Loss: 0.1021252 Vali Loss: 0.1786360 Test Loss: 0.1034666\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.0612155\n",
      "\tspeed: 0.2300s/iter; left time: 15266.6282s\n",
      "\titers: 200, epoch: 14 | loss: 0.1377859\n",
      "\tspeed: 0.0801s/iter; left time: 5306.7074s\n",
      "\titers: 300, epoch: 14 | loss: 0.0889226\n",
      "\tspeed: 0.0810s/iter; left time: 5359.7976s\n",
      "\titers: 400, epoch: 14 | loss: 0.0948551\n",
      "\tspeed: 0.0805s/iter; left time: 5320.7637s\n",
      "\titers: 500, epoch: 14 | loss: 0.0881059\n",
      "\tspeed: 0.0742s/iter; left time: 4897.8789s\n",
      "\titers: 600, epoch: 14 | loss: 0.0968326\n",
      "\tspeed: 0.0731s/iter; left time: 4818.2679s\n",
      "\titers: 700, epoch: 14 | loss: 0.1498471\n",
      "\tspeed: 0.0796s/iter; left time: 5236.0830s\n",
      "Epoch: 14 cost time: 60.043232679367065\n",
      "Epoch: 14, Steps: 764 | Train Loss: 0.1018716 Vali Loss: 0.1800991 Test Loss: 0.1024980\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1213759\n",
      "\tspeed: 0.2341s/iter; left time: 15357.6943s\n",
      "\titers: 200, epoch: 15 | loss: 0.0826379\n",
      "\tspeed: 0.0805s/iter; left time: 5275.2146s\n",
      "\titers: 300, epoch: 15 | loss: 0.0845426\n",
      "\tspeed: 0.0812s/iter; left time: 5311.4412s\n",
      "\titers: 400, epoch: 15 | loss: 0.0792814\n",
      "\tspeed: 0.0802s/iter; left time: 5237.0325s\n",
      "\titers: 500, epoch: 15 | loss: 0.1208850\n",
      "\tspeed: 0.0721s/iter; left time: 4704.1133s\n",
      "\titers: 600, epoch: 15 | loss: 0.0876196\n",
      "\tspeed: 0.0769s/iter; left time: 5005.8326s\n",
      "\titers: 700, epoch: 15 | loss: 0.1181504\n",
      "\tspeed: 0.0809s/iter; left time: 5257.2463s\n",
      "Epoch: 15 cost time: 60.30431270599365\n",
      "Epoch: 15, Steps: 764 | Train Loss: 0.1017419 Vali Loss: 0.1779727 Test Loss: 0.1043880\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1008058\n",
      "\tspeed: 0.2292s/iter; left time: 14863.4502s\n",
      "\titers: 200, epoch: 16 | loss: 0.1005515\n",
      "\tspeed: 0.0804s/iter; left time: 5207.1259s\n",
      "\titers: 300, epoch: 16 | loss: 0.1195518\n",
      "\tspeed: 0.0794s/iter; left time: 5133.8660s\n",
      "\titers: 400, epoch: 16 | loss: 0.1150640\n",
      "\tspeed: 0.0787s/iter; left time: 5079.4962s\n",
      "\titers: 500, epoch: 16 | loss: 0.1016699\n",
      "\tspeed: 0.0711s/iter; left time: 4582.6100s\n",
      "\titers: 600, epoch: 16 | loss: 0.1276093\n",
      "\tspeed: 0.0789s/iter; left time: 5078.8146s\n",
      "\titers: 700, epoch: 16 | loss: 0.1558163\n",
      "\tspeed: 0.0807s/iter; left time: 5184.1381s\n",
      "Epoch: 16 cost time: 60.117124795913696\n",
      "Epoch: 16, Steps: 764 | Train Loss: 0.1022087 Vali Loss: 0.1796290 Test Loss: 0.1034883\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1046095\n",
      "\tspeed: 0.2325s/iter; left time: 14900.5209s\n",
      "\titers: 200, epoch: 17 | loss: 0.1281656\n",
      "\tspeed: 0.0808s/iter; left time: 5171.8963s\n",
      "\titers: 300, epoch: 17 | loss: 0.0900563\n",
      "\tspeed: 0.0800s/iter; left time: 5109.6858s\n",
      "\titers: 400, epoch: 17 | loss: 0.0808184\n",
      "\tspeed: 0.0743s/iter; left time: 4735.8750s\n",
      "\titers: 500, epoch: 17 | loss: 0.1143519\n",
      "\tspeed: 0.0715s/iter; left time: 4556.0321s\n",
      "\titers: 600, epoch: 17 | loss: 0.0957268\n",
      "\tspeed: 0.0809s/iter; left time: 5142.6218s\n",
      "\titers: 700, epoch: 17 | loss: 0.0810783\n",
      "\tspeed: 0.0802s/iter; left time: 5093.0476s\n",
      "Epoch: 17 cost time: 59.98435664176941\n",
      "Epoch: 17, Steps: 764 | Train Loss: 0.1020563 Vali Loss: 0.1775755 Test Loss: 0.1030553\n",
      "Validation loss decreased (0.177764 --> 0.177576).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 18 | loss: 0.1234444\n",
      "\tspeed: 0.2445s/iter; left time: 15479.3120s\n",
      "\titers: 200, epoch: 18 | loss: 0.1098441\n",
      "\tspeed: 0.0805s/iter; left time: 5089.8906s\n",
      "\titers: 300, epoch: 18 | loss: 0.0997116\n",
      "\tspeed: 0.0799s/iter; left time: 5042.2036s\n",
      "\titers: 400, epoch: 18 | loss: 0.0856101\n",
      "\tspeed: 0.0712s/iter; left time: 4486.0419s\n",
      "\titers: 500, epoch: 18 | loss: 0.0967506\n",
      "\tspeed: 0.0779s/iter; left time: 4901.5233s\n",
      "\titers: 600, epoch: 18 | loss: 0.0914463\n",
      "\tspeed: 0.0799s/iter; left time: 5018.4598s\n",
      "\titers: 700, epoch: 18 | loss: 0.0974954\n",
      "\tspeed: 0.0811s/iter; left time: 5084.3951s\n",
      "Epoch: 18 cost time: 60.36162972450256\n",
      "Epoch: 18, Steps: 764 | Train Loss: 0.1016598 Vali Loss: 0.1789832 Test Loss: 0.1031939\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 19 | loss: 0.1098575\n",
      "\tspeed: 0.2332s/iter; left time: 14584.6147s\n",
      "\titers: 200, epoch: 19 | loss: 0.0964803\n",
      "\tspeed: 0.0802s/iter; left time: 5005.9570s\n",
      "\titers: 300, epoch: 19 | loss: 0.1060707\n",
      "\tspeed: 0.0773s/iter; left time: 4819.1132s\n",
      "\titers: 400, epoch: 19 | loss: 0.0902606\n",
      "\tspeed: 0.0721s/iter; left time: 4490.0050s\n",
      "\titers: 500, epoch: 19 | loss: 0.0959691\n",
      "\tspeed: 0.0801s/iter; left time: 4975.8976s\n",
      "\titers: 600, epoch: 19 | loss: 0.0715437\n",
      "\tspeed: 0.0811s/iter; left time: 5029.3719s\n",
      "\titers: 700, epoch: 19 | loss: 0.0886579\n",
      "\tspeed: 0.0810s/iter; left time: 5018.7578s\n",
      "Epoch: 19 cost time: 60.44335579872131\n",
      "Epoch: 19, Steps: 764 | Train Loss: 0.1020098 Vali Loss: 0.1783942 Test Loss: 0.1032182\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 3.814697265625e-09\n",
      "\titers: 100, epoch: 20 | loss: 0.0777919\n",
      "\tspeed: 0.2291s/iter; left time: 14152.3442s\n",
      "\titers: 200, epoch: 20 | loss: 0.1409278\n",
      "\tspeed: 0.0803s/iter; left time: 4953.5701s\n",
      "\titers: 300, epoch: 20 | loss: 0.0718714\n",
      "\tspeed: 0.0732s/iter; left time: 4507.1617s\n",
      "\titers: 400, epoch: 20 | loss: 0.1084507\n",
      "\tspeed: 0.0725s/iter; left time: 4459.0667s\n",
      "\titers: 500, epoch: 20 | loss: 0.0754917\n",
      "\tspeed: 0.0808s/iter; left time: 4961.2136s\n",
      "\titers: 600, epoch: 20 | loss: 0.1556426\n",
      "\tspeed: 0.0802s/iter; left time: 4916.0691s\n",
      "\titers: 700, epoch: 20 | loss: 0.1572414\n",
      "\tspeed: 0.0802s/iter; left time: 4907.8142s\n",
      "Epoch: 20 cost time: 59.917378664016724\n",
      "Epoch: 20, Steps: 764 | Train Loss: 0.1020968 Vali Loss: 0.1781761 Test Loss: 0.1040754\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.9073486328125e-09\n",
      "\titers: 100, epoch: 21 | loss: 0.0942153\n",
      "\tspeed: 0.2299s/iter; left time: 14031.5903s\n",
      "\titers: 200, epoch: 21 | loss: 0.0930700\n",
      "\tspeed: 0.0806s/iter; left time: 4908.4338s\n",
      "\titers: 300, epoch: 21 | loss: 0.0872205\n",
      "\tspeed: 0.0732s/iter; left time: 4450.2948s\n",
      "\titers: 400, epoch: 21 | loss: 0.0919478\n",
      "\tspeed: 0.0755s/iter; left time: 4583.9653s\n",
      "\titers: 500, epoch: 21 | loss: 0.1135027\n",
      "\tspeed: 0.0809s/iter; left time: 4902.0210s\n",
      "\titers: 600, epoch: 21 | loss: 0.0922311\n",
      "\tspeed: 0.0798s/iter; left time: 4832.5777s\n",
      "\titers: 700, epoch: 21 | loss: 0.0757931\n",
      "\tspeed: 0.0805s/iter; left time: 4861.1658s\n",
      "Epoch: 21 cost time: 60.29018807411194\n",
      "Epoch: 21, Steps: 764 | Train Loss: 0.1021073 Vali Loss: 0.1789140 Test Loss: 0.1032714\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 9.5367431640625e-10\n",
      "\titers: 100, epoch: 22 | loss: 0.0817669\n",
      "\tspeed: 0.2297s/iter; left time: 13839.2401s\n",
      "\titers: 200, epoch: 22 | loss: 0.1669648\n",
      "\tspeed: 0.0796s/iter; left time: 4786.4449s\n",
      "\titers: 300, epoch: 22 | loss: 0.0809543\n",
      "\tspeed: 0.0723s/iter; left time: 4344.5315s\n",
      "\titers: 400, epoch: 22 | loss: 0.1054696\n",
      "\tspeed: 0.0789s/iter; left time: 4728.2913s\n",
      "\titers: 500, epoch: 22 | loss: 0.0935417\n",
      "\tspeed: 0.0778s/iter; left time: 4656.2705s\n",
      "\titers: 600, epoch: 22 | loss: 0.0893052\n",
      "\tspeed: 0.0786s/iter; left time: 4695.5885s\n",
      "\titers: 700, epoch: 22 | loss: 0.0833920\n",
      "\tspeed: 0.0778s/iter; left time: 4639.6246s\n",
      "Epoch: 22 cost time: 59.50896978378296\n",
      "Epoch: 22, Steps: 764 | Train Loss: 0.1021741 Vali Loss: 0.1783189 Test Loss: 0.1039946\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 4.76837158203125e-10\n",
      "\titers: 100, epoch: 23 | loss: 0.1475677\n",
      "\tspeed: 0.2207s/iter; left time: 13131.9696s\n",
      "\titers: 200, epoch: 23 | loss: 0.0883570\n",
      "\tspeed: 0.0753s/iter; left time: 4470.8338s\n",
      "\titers: 300, epoch: 23 | loss: 0.0852722\n",
      "\tspeed: 0.0714s/iter; left time: 4234.5241s\n",
      "\titers: 400, epoch: 23 | loss: 0.1019097\n",
      "\tspeed: 0.0782s/iter; left time: 4631.4218s\n",
      "\titers: 500, epoch: 23 | loss: 0.1106148\n",
      "\tspeed: 0.0786s/iter; left time: 4645.7272s\n",
      "\titers: 600, epoch: 23 | loss: 0.0979195\n",
      "\tspeed: 0.0787s/iter; left time: 4640.8083s\n",
      "\titers: 700, epoch: 23 | loss: 0.1158627\n",
      "\tspeed: 0.0785s/iter; left time: 4620.6298s\n",
      "Epoch: 23 cost time: 58.929177045822144\n",
      "Epoch: 23, Steps: 764 | Train Loss: 0.1019289 Vali Loss: 0.1774316 Test Loss: 0.1037690\n",
      "Validation loss decreased (0.177576 --> 0.177432).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-10\n",
      "\titers: 100, epoch: 24 | loss: 0.1596545\n",
      "\tspeed: 0.2319s/iter; left time: 13621.9977s\n",
      "\titers: 200, epoch: 24 | loss: 0.0878069\n",
      "\tspeed: 0.0726s/iter; left time: 4257.6384s\n",
      "\titers: 300, epoch: 24 | loss: 0.0897842\n",
      "\tspeed: 0.0747s/iter; left time: 4374.4309s\n",
      "\titers: 400, epoch: 24 | loss: 0.0856142\n",
      "\tspeed: 0.0776s/iter; left time: 4535.8527s\n",
      "\titers: 500, epoch: 24 | loss: 0.1183400\n",
      "\tspeed: 0.0787s/iter; left time: 4589.0806s\n",
      "\titers: 600, epoch: 24 | loss: 0.1288895\n",
      "\tspeed: 0.0800s/iter; left time: 4659.3694s\n",
      "\titers: 700, epoch: 24 | loss: 0.0797768\n",
      "\tspeed: 0.0789s/iter; left time: 4584.7277s\n",
      "Epoch: 24 cost time: 59.10062646865845\n",
      "Epoch: 24, Steps: 764 | Train Loss: 0.1020788 Vali Loss: 0.1782837 Test Loss: 0.1031286\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 1.1920928955078125e-10\n",
      "\titers: 100, epoch: 25 | loss: 0.0893111\n",
      "\tspeed: 0.2206s/iter; left time: 12788.5202s\n",
      "\titers: 200, epoch: 25 | loss: 0.1006225\n",
      "\tspeed: 0.0712s/iter; left time: 4118.5665s\n",
      "\titers: 300, epoch: 25 | loss: 0.1208407\n",
      "\tspeed: 0.0768s/iter; left time: 4437.9591s\n",
      "\titers: 400, epoch: 25 | loss: 0.1295927\n",
      "\tspeed: 0.0787s/iter; left time: 4539.8073s\n",
      "\titers: 500, epoch: 25 | loss: 0.0894179\n",
      "\tspeed: 0.0795s/iter; left time: 4579.1082s\n",
      "\titers: 600, epoch: 25 | loss: 0.1155873\n",
      "\tspeed: 0.0790s/iter; left time: 4539.8708s\n",
      "\titers: 700, epoch: 25 | loss: 0.1028100\n",
      "\tspeed: 0.0787s/iter; left time: 4512.9202s\n",
      "Epoch: 25 cost time: 59.29305958747864\n",
      "Epoch: 25, Steps: 764 | Train Loss: 0.1018737 Vali Loss: 0.1778468 Test Loss: 0.1039265\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 5.960464477539063e-11\n",
      "\titers: 100, epoch: 26 | loss: 0.0936329\n",
      "\tspeed: 0.2155s/iter; left time: 12327.3776s\n",
      "\titers: 200, epoch: 26 | loss: 0.0987848\n",
      "\tspeed: 0.0705s/iter; left time: 4024.3597s\n",
      "\titers: 300, epoch: 26 | loss: 0.1305929\n",
      "\tspeed: 0.0782s/iter; left time: 4458.8497s\n",
      "\titers: 400, epoch: 26 | loss: 0.1298330\n",
      "\tspeed: 0.0775s/iter; left time: 4407.0548s\n",
      "\titers: 500, epoch: 26 | loss: 0.0991739\n",
      "\tspeed: 0.0784s/iter; left time: 4451.3857s\n",
      "\titers: 600, epoch: 26 | loss: 0.0828410\n",
      "\tspeed: 0.0783s/iter; left time: 4437.3347s\n",
      "\titers: 700, epoch: 26 | loss: 0.0962443\n",
      "\tspeed: 0.0787s/iter; left time: 4455.1921s\n",
      "Epoch: 26 cost time: 58.8785195350647\n",
      "Epoch: 26, Steps: 764 | Train Loss: 0.1018921 Vali Loss: 0.1785262 Test Loss: 0.1039031\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 2.980232238769531e-11\n",
      "\titers: 100, epoch: 27 | loss: 0.0879817\n",
      "\tspeed: 0.2165s/iter; left time: 12219.1648s\n",
      "\titers: 200, epoch: 27 | loss: 0.0918186\n",
      "\tspeed: 0.0722s/iter; left time: 4066.8882s\n",
      "\titers: 300, epoch: 27 | loss: 0.1350571\n",
      "\tspeed: 0.0796s/iter; left time: 4478.2623s\n",
      "\titers: 400, epoch: 27 | loss: 0.1000142\n",
      "\tspeed: 0.0784s/iter; left time: 4398.9496s\n",
      "\titers: 500, epoch: 27 | loss: 0.0939329\n",
      "\tspeed: 0.0789s/iter; left time: 4419.6934s\n",
      "\titers: 600, epoch: 27 | loss: 0.1148936\n",
      "\tspeed: 0.0771s/iter; left time: 4314.0606s\n",
      "\titers: 700, epoch: 27 | loss: 0.1078584\n",
      "\tspeed: 0.0787s/iter; left time: 4394.0184s\n",
      "Epoch: 27 cost time: 59.00806546211243\n",
      "Epoch: 27, Steps: 764 | Train Loss: 0.1019457 Vali Loss: 0.1768999 Test Loss: 0.1042310\n",
      "Validation loss decreased (0.177432 --> 0.176900).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-11\n",
      "\titers: 100, epoch: 28 | loss: 0.0949211\n",
      "\tspeed: 0.2218s/iter; left time: 12347.9622s\n",
      "\titers: 200, epoch: 28 | loss: 0.1342201\n",
      "\tspeed: 0.0754s/iter; left time: 4188.5557s\n",
      "\titers: 300, epoch: 28 | loss: 0.1155812\n",
      "\tspeed: 0.0778s/iter; left time: 4318.1023s\n",
      "\titers: 400, epoch: 28 | loss: 0.0975508\n",
      "\tspeed: 0.0774s/iter; left time: 4283.7601s\n",
      "\titers: 500, epoch: 28 | loss: 0.0903129\n",
      "\tspeed: 0.0781s/iter; left time: 4316.9571s\n",
      "\titers: 600, epoch: 28 | loss: 0.1200657\n",
      "\tspeed: 0.0779s/iter; left time: 4298.2733s\n",
      "\titers: 700, epoch: 28 | loss: 0.1312356\n",
      "\tspeed: 0.0779s/iter; left time: 4289.6393s\n",
      "Epoch: 28 cost time: 58.621275901794434\n",
      "Epoch: 28, Steps: 764 | Train Loss: 0.1021142 Vali Loss: 0.1784618 Test Loss: 0.1041943\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 7.450580596923828e-12\n",
      "\titers: 100, epoch: 29 | loss: 0.1272597\n",
      "\tspeed: 0.2156s/iter; left time: 11840.4932s\n",
      "\titers: 200, epoch: 29 | loss: 0.1168119\n",
      "\tspeed: 0.0774s/iter; left time: 4240.3335s\n",
      "\titers: 300, epoch: 29 | loss: 0.1044759\n",
      "\tspeed: 0.0810s/iter; left time: 4429.6866s\n",
      "\titers: 400, epoch: 29 | loss: 0.0804077\n",
      "\tspeed: 0.0795s/iter; left time: 4343.4656s\n",
      "\titers: 500, epoch: 29 | loss: 0.1015990\n",
      "\tspeed: 0.0800s/iter; left time: 4359.5804s\n",
      "\titers: 600, epoch: 29 | loss: 0.0821883\n",
      "\tspeed: 0.0807s/iter; left time: 4389.2897s\n",
      "\titers: 700, epoch: 29 | loss: 0.0791833\n",
      "\tspeed: 0.0805s/iter; left time: 4372.2054s\n",
      "Epoch: 29 cost time: 60.35013198852539\n",
      "Epoch: 29, Steps: 764 | Train Loss: 0.1020293 Vali Loss: 0.1799934 Test Loss: 0.1026172\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 3.725290298461914e-12\n",
      "\titers: 100, epoch: 30 | loss: 0.0962022\n",
      "\tspeed: 0.2117s/iter; left time: 11462.9606s\n",
      "\titers: 200, epoch: 30 | loss: 0.1216132\n",
      "\tspeed: 0.0789s/iter; left time: 4264.1053s\n",
      "\titers: 300, epoch: 30 | loss: 0.0959561\n",
      "\tspeed: 0.0808s/iter; left time: 4357.0833s\n",
      "\titers: 400, epoch: 30 | loss: 0.1116980\n",
      "\tspeed: 0.0792s/iter; left time: 4265.7473s\n",
      "\titers: 500, epoch: 30 | loss: 0.0908092\n",
      "\tspeed: 0.0791s/iter; left time: 4249.1759s\n",
      "\titers: 600, epoch: 30 | loss: 0.1102756\n",
      "\tspeed: 0.0800s/iter; left time: 4290.3640s\n",
      "\titers: 700, epoch: 30 | loss: 0.0932194\n",
      "\tspeed: 0.0794s/iter; left time: 4252.2920s\n",
      "Epoch: 30 cost time: 60.157479763031006\n",
      "Epoch: 30, Steps: 764 | Train Loss: 0.1017803 Vali Loss: 0.1783652 Test Loss: 0.1042200\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.862645149230957e-12\n",
      "\titers: 100, epoch: 31 | loss: 0.1176916\n",
      "\tspeed: 0.2162s/iter; left time: 11542.4357s\n",
      "\titers: 200, epoch: 31 | loss: 0.1260930\n",
      "\tspeed: 0.0786s/iter; left time: 4185.5935s\n",
      "\titers: 300, epoch: 31 | loss: 0.0883562\n",
      "\tspeed: 0.0803s/iter; left time: 4268.1949s\n",
      "\titers: 400, epoch: 31 | loss: 0.0882967\n",
      "\tspeed: 0.0787s/iter; left time: 4177.0895s\n",
      "\titers: 500, epoch: 31 | loss: 0.0863572\n",
      "\tspeed: 0.0786s/iter; left time: 4164.8308s\n",
      "\titers: 600, epoch: 31 | loss: 0.1206050\n",
      "\tspeed: 0.0786s/iter; left time: 4154.1256s\n",
      "\titers: 700, epoch: 31 | loss: 0.1291562\n",
      "\tspeed: 0.0798s/iter; left time: 4212.2690s\n",
      "Epoch: 31 cost time: 60.463340759277344\n",
      "Epoch: 31, Steps: 764 | Train Loss: 0.1019591 Vali Loss: 0.1786530 Test Loss: 0.1040089\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 9.313225746154785e-13\n",
      "\titers: 100, epoch: 32 | loss: 0.0912568\n",
      "\tspeed: 0.2125s/iter; left time: 11181.7395s\n",
      "\titers: 200, epoch: 32 | loss: 0.0646382\n",
      "\tspeed: 0.0801s/iter; left time: 4204.7273s\n",
      "\titers: 300, epoch: 32 | loss: 0.1192873\n",
      "\tspeed: 0.0800s/iter; left time: 4194.4936s\n",
      "\titers: 400, epoch: 32 | loss: 0.1058789\n",
      "\tspeed: 0.0799s/iter; left time: 4178.8145s\n",
      "\titers: 500, epoch: 32 | loss: 0.0906314\n",
      "\tspeed: 0.0796s/iter; left time: 4155.1872s\n",
      "\titers: 600, epoch: 32 | loss: 0.1451554\n",
      "\tspeed: 0.0809s/iter; left time: 4217.6884s\n",
      "\titers: 700, epoch: 32 | loss: 0.1176491\n",
      "\tspeed: 0.0799s/iter; left time: 4157.1168s\n",
      "Epoch: 32 cost time: 60.95684623718262\n",
      "Epoch: 32, Steps: 764 | Train Loss: 0.1020279 Vali Loss: 0.1784863 Test Loss: 0.1044354\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 4.656612873077393e-13\n",
      "\titers: 100, epoch: 33 | loss: 0.1023695\n",
      "\tspeed: 0.2207s/iter; left time: 11442.7880s\n",
      "\titers: 200, epoch: 33 | loss: 0.0844683\n",
      "\tspeed: 0.0804s/iter; left time: 4163.2105s\n",
      "\titers: 300, epoch: 33 | loss: 0.0820435\n",
      "\tspeed: 0.0802s/iter; left time: 4143.2804s\n",
      "\titers: 400, epoch: 33 | loss: 0.1394023\n",
      "\tspeed: 0.0808s/iter; left time: 4163.0732s\n",
      "\titers: 500, epoch: 33 | loss: 0.1181449\n",
      "\tspeed: 0.0810s/iter; left time: 4168.7076s\n",
      "\titers: 600, epoch: 33 | loss: 0.0811194\n",
      "\tspeed: 0.0780s/iter; left time: 4005.9003s\n",
      "\titers: 700, epoch: 33 | loss: 0.0713815\n",
      "\tspeed: 0.0789s/iter; left time: 4044.6546s\n",
      "Epoch: 33 cost time: 60.737715005874634\n",
      "Epoch: 33, Steps: 764 | Train Loss: 0.1018429 Vali Loss: 0.1780512 Test Loss: 0.1039163\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 2.3283064365386963e-13\n",
      "\titers: 100, epoch: 34 | loss: 0.1199491\n",
      "\tspeed: 0.2078s/iter; left time: 10617.1781s\n",
      "\titers: 200, epoch: 34 | loss: 0.1314830\n",
      "\tspeed: 0.0798s/iter; left time: 4068.7270s\n",
      "\titers: 300, epoch: 34 | loss: 0.1183944\n",
      "\tspeed: 0.0809s/iter; left time: 4117.8094s\n",
      "\titers: 400, epoch: 34 | loss: 0.1376156\n",
      "\tspeed: 0.0800s/iter; left time: 4061.0605s\n",
      "\titers: 500, epoch: 34 | loss: 0.0911163\n",
      "\tspeed: 0.0799s/iter; left time: 4047.6621s\n",
      "\titers: 600, epoch: 34 | loss: 0.0895938\n",
      "\tspeed: 0.0801s/iter; left time: 4052.7867s\n",
      "\titers: 700, epoch: 34 | loss: 0.1047649\n",
      "\tspeed: 0.0823s/iter; left time: 4156.2407s\n",
      "Epoch: 34 cost time: 60.89063906669617\n",
      "Epoch: 34, Steps: 764 | Train Loss: 0.1018652 Vali Loss: 0.1790395 Test Loss: 0.1035753\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 1.1641532182693482e-13\n",
      "\titers: 100, epoch: 35 | loss: 0.1202872\n",
      "\tspeed: 0.2152s/iter; left time: 10829.6108s\n",
      "\titers: 200, epoch: 35 | loss: 0.0867886\n",
      "\tspeed: 0.0829s/iter; left time: 4164.7883s\n",
      "\titers: 300, epoch: 35 | loss: 0.0774769\n",
      "\tspeed: 0.0799s/iter; left time: 4004.8680s\n",
      "\titers: 400, epoch: 35 | loss: 0.1051917\n",
      "\tspeed: 0.0799s/iter; left time: 3995.1447s\n",
      "\titers: 500, epoch: 35 | loss: 0.0824951\n",
      "\tspeed: 0.0812s/iter; left time: 4054.7793s\n",
      "\titers: 600, epoch: 35 | loss: 0.0878220\n",
      "\tspeed: 0.0799s/iter; left time: 3979.3513s\n",
      "\titers: 700, epoch: 35 | loss: 0.0994710\n",
      "\tspeed: 0.0768s/iter; left time: 3816.5966s\n",
      "Epoch: 35 cost time: 60.681411027908325\n",
      "Epoch: 35, Steps: 764 | Train Loss: 0.1019770 Vali Loss: 0.1784496 Test Loss: 0.1029521\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 5.820766091346741e-14\n",
      "\titers: 100, epoch: 36 | loss: 0.0786834\n",
      "\tspeed: 0.2208s/iter; left time: 10944.5815s\n",
      "\titers: 200, epoch: 36 | loss: 0.1440179\n",
      "\tspeed: 0.0791s/iter; left time: 3912.2206s\n",
      "\titers: 300, epoch: 36 | loss: 0.0872145\n",
      "\tspeed: 0.0806s/iter; left time: 3979.0266s\n",
      "\titers: 400, epoch: 36 | loss: 0.0971873\n",
      "\tspeed: 0.0791s/iter; left time: 3896.6140s\n",
      "\titers: 500, epoch: 36 | loss: 0.1026474\n",
      "\tspeed: 0.0787s/iter; left time: 3870.9801s\n",
      "\titers: 600, epoch: 36 | loss: 0.0886753\n",
      "\tspeed: 0.0787s/iter; left time: 3861.0684s\n",
      "\titers: 700, epoch: 36 | loss: 0.1094558\n",
      "\tspeed: 0.0738s/iter; left time: 3613.8246s\n",
      "Epoch: 36 cost time: 59.47753953933716\n",
      "Epoch: 36, Steps: 764 | Train Loss: 0.1017990 Vali Loss: 0.1778449 Test Loss: 0.1035982\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 2.9103830456733704e-14\n",
      "\titers: 100, epoch: 37 | loss: 0.0753697\n",
      "\tspeed: 0.2130s/iter; left time: 10395.8726s\n",
      "\titers: 200, epoch: 37 | loss: 0.0737105\n",
      "\tspeed: 0.0777s/iter; left time: 3784.2533s\n",
      "\titers: 300, epoch: 37 | loss: 0.0929474\n",
      "\tspeed: 0.0784s/iter; left time: 3810.5118s\n",
      "\titers: 400, epoch: 37 | loss: 0.0848978\n",
      "\tspeed: 0.0773s/iter; left time: 3748.6907s\n",
      "\titers: 500, epoch: 37 | loss: 0.1577922\n",
      "\tspeed: 0.0791s/iter; left time: 3827.4950s\n",
      "\titers: 600, epoch: 37 | loss: 0.0952408\n",
      "\tspeed: 0.0806s/iter; left time: 3890.5758s\n",
      "\titers: 700, epoch: 37 | loss: 0.1106353\n",
      "\tspeed: 0.0730s/iter; left time: 3517.2522s\n",
      "Epoch: 37 cost time: 59.321146965026855\n",
      "Epoch: 37, Steps: 764 | Train Loss: 0.1019902 Vali Loss: 0.1783211 Test Loss: 0.1033387\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 1.4551915228366852e-14\n",
      "\titers: 100, epoch: 38 | loss: 0.0944421\n",
      "\tspeed: 0.2178s/iter; left time: 10461.0804s\n",
      "\titers: 200, epoch: 38 | loss: 0.0829500\n",
      "\tspeed: 0.0809s/iter; left time: 3878.0672s\n",
      "\titers: 300, epoch: 38 | loss: 0.1045147\n",
      "\tspeed: 0.0792s/iter; left time: 3787.3055s\n",
      "\titers: 400, epoch: 38 | loss: 0.1027267\n",
      "\tspeed: 0.0807s/iter; left time: 3853.4829s\n",
      "\titers: 500, epoch: 38 | loss: 0.0838822\n",
      "\tspeed: 0.0783s/iter; left time: 3729.3221s\n",
      "\titers: 600, epoch: 38 | loss: 0.0777697\n",
      "\tspeed: 0.0804s/iter; left time: 3819.3207s\n",
      "\titers: 700, epoch: 38 | loss: 0.1086020\n",
      "\tspeed: 0.0747s/iter; left time: 3544.8674s\n",
      "Epoch: 38 cost time: 59.48441290855408\n",
      "Epoch: 38, Steps: 764 | Train Loss: 0.1017532 Vali Loss: 0.1773867 Test Loss: 0.1045118\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 7.275957614183426e-15\n",
      "\titers: 100, epoch: 39 | loss: 0.0818712\n",
      "\tspeed: 0.2132s/iter; left time: 10076.9765s\n",
      "\titers: 200, epoch: 39 | loss: 0.0765228\n",
      "\tspeed: 0.0797s/iter; left time: 3759.6599s\n",
      "\titers: 300, epoch: 39 | loss: 0.1020816\n",
      "\tspeed: 0.0807s/iter; left time: 3800.1397s\n",
      "\titers: 400, epoch: 39 | loss: 0.0973093\n",
      "\tspeed: 0.0809s/iter; left time: 3800.9222s\n",
      "\titers: 500, epoch: 39 | loss: 0.1247125\n",
      "\tspeed: 0.0797s/iter; left time: 3735.5671s\n",
      "\titers: 600, epoch: 39 | loss: 0.1077897\n",
      "\tspeed: 0.0785s/iter; left time: 3670.5643s\n",
      "\titers: 700, epoch: 39 | loss: 0.1501662\n",
      "\tspeed: 0.0734s/iter; left time: 3426.3872s\n",
      "Epoch: 39 cost time: 60.42103934288025\n",
      "Epoch: 39, Steps: 764 | Train Loss: 0.1020380 Vali Loss: 0.1793979 Test Loss: 0.1038380\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 3.637978807091713e-15\n",
      "\titers: 100, epoch: 40 | loss: 0.1531720\n",
      "\tspeed: 0.2291s/iter; left time: 10652.5977s\n",
      "\titers: 200, epoch: 40 | loss: 0.0803307\n",
      "\tspeed: 0.0807s/iter; left time: 3744.1390s\n",
      "\titers: 300, epoch: 40 | loss: 0.1293717\n",
      "\tspeed: 0.0809s/iter; left time: 3744.9802s\n",
      "\titers: 400, epoch: 40 | loss: 0.1088643\n",
      "\tspeed: 0.0802s/iter; left time: 3707.7378s\n",
      "\titers: 500, epoch: 40 | loss: 0.0958144\n",
      "\tspeed: 0.0800s/iter; left time: 3688.9729s\n",
      "\titers: 600, epoch: 40 | loss: 0.0720157\n",
      "\tspeed: 0.0746s/iter; left time: 3432.2069s\n",
      "\titers: 700, epoch: 40 | loss: 0.1205950\n",
      "\tspeed: 0.0768s/iter; left time: 3527.7519s\n",
      "Epoch: 40 cost time: 60.43707799911499\n",
      "Epoch: 40, Steps: 764 | Train Loss: 0.1019544 Vali Loss: 0.1780214 Test Loss: 0.1041281\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 1.8189894035458565e-15\n",
      "\titers: 100, epoch: 41 | loss: 0.1124788\n",
      "\tspeed: 0.2208s/iter; left time: 10101.7331s\n",
      "\titers: 200, epoch: 41 | loss: 0.0801397\n",
      "\tspeed: 0.0799s/iter; left time: 3644.9637s\n",
      "\titers: 300, epoch: 41 | loss: 0.0758583\n",
      "\tspeed: 0.0788s/iter; left time: 3590.3162s\n",
      "\titers: 400, epoch: 41 | loss: 0.1042244\n",
      "\tspeed: 0.0787s/iter; left time: 3577.8069s\n",
      "\titers: 500, epoch: 41 | loss: 0.1076945\n",
      "\tspeed: 0.0787s/iter; left time: 3570.2872s\n",
      "\titers: 600, epoch: 41 | loss: 0.0836373\n",
      "\tspeed: 0.0735s/iter; left time: 3324.7126s\n",
      "\titers: 700, epoch: 41 | loss: 0.0974256\n",
      "\tspeed: 0.0783s/iter; left time: 3533.3320s\n",
      "Epoch: 41 cost time: 59.82418775558472\n",
      "Epoch: 41, Steps: 764 | Train Loss: 0.1020896 Vali Loss: 0.1785170 Test Loss: 0.1033732\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Updating learning rate to 9.094947017729283e-16\n",
      "\titers: 100, epoch: 42 | loss: 0.0828434\n",
      "\tspeed: 0.2242s/iter; left time: 10083.1575s\n",
      "\titers: 200, epoch: 42 | loss: 0.1150156\n",
      "\tspeed: 0.0833s/iter; left time: 3737.6134s\n",
      "\titers: 300, epoch: 42 | loss: 0.0948550\n",
      "\tspeed: 0.0819s/iter; left time: 3668.4011s\n",
      "\titers: 400, epoch: 42 | loss: 0.1286493\n",
      "\tspeed: 0.0808s/iter; left time: 3610.1123s\n",
      "\titers: 500, epoch: 42 | loss: 0.1091421\n",
      "\tspeed: 0.0763s/iter; left time: 3400.8145s\n",
      "\titers: 600, epoch: 42 | loss: 0.1025915\n",
      "\tspeed: 0.0728s/iter; left time: 3236.2650s\n",
      "\titers: 700, epoch: 42 | loss: 0.0974067\n",
      "\tspeed: 0.0804s/iter; left time: 3569.5937s\n",
      "Epoch: 42 cost time: 60.7699408531189\n",
      "Epoch: 42, Steps: 764 | Train Loss: 0.1020719 Vali Loss: 0.1790541 Test Loss: 0.1025107\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Updating learning rate to 4.547473508864641e-16\n",
      "\titers: 100, epoch: 43 | loss: 0.0804606\n",
      "\tspeed: 0.2334s/iter; left time: 10320.9220s\n",
      "\titers: 200, epoch: 43 | loss: 0.1393336\n",
      "\tspeed: 0.0795s/iter; left time: 3506.9941s\n",
      "\titers: 300, epoch: 43 | loss: 0.0936525\n",
      "\tspeed: 0.0789s/iter; left time: 3473.1993s\n",
      "\titers: 400, epoch: 43 | loss: 0.0727792\n",
      "\tspeed: 0.0789s/iter; left time: 3463.6229s\n",
      "\titers: 500, epoch: 43 | loss: 0.0910740\n",
      "\tspeed: 0.0706s/iter; left time: 3094.8510s\n",
      "\titers: 600, epoch: 43 | loss: 0.0898461\n",
      "\tspeed: 0.0754s/iter; left time: 3297.8874s\n",
      "\titers: 700, epoch: 43 | loss: 0.0858282\n",
      "\tspeed: 0.0799s/iter; left time: 3483.5580s\n",
      "Epoch: 43 cost time: 59.6510853767395\n",
      "Epoch: 43, Steps: 764 | Train Loss: 0.1019311 Vali Loss: 0.1781079 Test Loss: 0.1028967\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Updating learning rate to 2.2737367544323206e-16\n",
      "\titers: 100, epoch: 44 | loss: 0.0751468\n",
      "\tspeed: 0.2227s/iter; left time: 9677.4248s\n",
      "\titers: 200, epoch: 44 | loss: 0.1130145\n",
      "\tspeed: 0.0779s/iter; left time: 3378.9750s\n",
      "\titers: 300, epoch: 44 | loss: 0.1047312\n",
      "\tspeed: 0.0794s/iter; left time: 3432.5517s\n",
      "\titers: 400, epoch: 44 | loss: 0.0754369\n",
      "\tspeed: 0.0794s/iter; left time: 3426.9987s\n",
      "\titers: 500, epoch: 44 | loss: 0.0925088\n",
      "\tspeed: 0.0746s/iter; left time: 3212.5850s\n",
      "\titers: 600, epoch: 44 | loss: 0.0817495\n",
      "\tspeed: 0.0795s/iter; left time: 3415.0760s\n",
      "\titers: 700, epoch: 44 | loss: 0.1268831\n",
      "\tspeed: 0.0800s/iter; left time: 3426.7078s\n",
      "Epoch: 44 cost time: 59.90483736991882\n",
      "Epoch: 44, Steps: 764 | Train Loss: 0.1018047 Vali Loss: 0.1775914 Test Loss: 0.1040793\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Updating learning rate to 1.1368683772161603e-16\n",
      "\titers: 100, epoch: 45 | loss: 0.1009628\n",
      "\tspeed: 0.2232s/iter; left time: 9529.0761s\n",
      "\titers: 200, epoch: 45 | loss: 0.0920866\n",
      "\tspeed: 0.0783s/iter; left time: 3336.0112s\n",
      "\titers: 300, epoch: 45 | loss: 0.0847585\n",
      "\tspeed: 0.0801s/iter; left time: 3403.1349s\n",
      "\titers: 400, epoch: 45 | loss: 0.1013403\n",
      "\tspeed: 0.0754s/iter; left time: 3197.7238s\n",
      "\titers: 500, epoch: 45 | loss: 0.0807522\n",
      "\tspeed: 0.0751s/iter; left time: 3173.6585s\n",
      "\titers: 600, epoch: 45 | loss: 0.0874930\n",
      "\tspeed: 0.0814s/iter; left time: 3433.9928s\n",
      "\titers: 700, epoch: 45 | loss: 0.1054065\n",
      "\tspeed: 0.0805s/iter; left time: 3386.7864s\n",
      "Epoch: 45 cost time: 60.01793074607849\n",
      "Epoch: 45, Steps: 764 | Train Loss: 0.1019675 Vali Loss: 0.1771483 Test Loss: 0.1044045\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Updating learning rate to 5.684341886080802e-17\n",
      "\titers: 100, epoch: 46 | loss: 0.1152181\n",
      "\tspeed: 0.2241s/iter; left time: 9395.6725s\n",
      "\titers: 200, epoch: 46 | loss: 0.1108586\n",
      "\tspeed: 0.0779s/iter; left time: 3259.8028s\n",
      "\titers: 300, epoch: 46 | loss: 0.0880682\n",
      "\tspeed: 0.0788s/iter; left time: 3285.9319s\n",
      "\titers: 400, epoch: 46 | loss: 0.0839860\n",
      "\tspeed: 0.0725s/iter; left time: 3016.4131s\n",
      "\titers: 500, epoch: 46 | loss: 0.0980472\n",
      "\tspeed: 0.0777s/iter; left time: 3224.5438s\n",
      "\titers: 600, epoch: 46 | loss: 0.1148228\n",
      "\tspeed: 0.0798s/iter; left time: 3307.0220s\n",
      "\titers: 700, epoch: 46 | loss: 0.0996619\n",
      "\tspeed: 0.0796s/iter; left time: 3290.6696s\n",
      "Epoch: 46 cost time: 59.7442901134491\n",
      "Epoch: 46, Steps: 764 | Train Loss: 0.1019531 Vali Loss: 0.1788400 Test Loss: 0.1027183\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Updating learning rate to 2.842170943040401e-17\n",
      "\titers: 100, epoch: 47 | loss: 0.1149533\n",
      "\tspeed: 0.2199s/iter; left time: 9050.5159s\n",
      "\titers: 200, epoch: 47 | loss: 0.1167247\n",
      "\tspeed: 0.0787s/iter; left time: 3232.5323s\n",
      "\titers: 300, epoch: 47 | loss: 0.0917285\n",
      "\tspeed: 0.0781s/iter; left time: 3197.2713s\n",
      "\titers: 400, epoch: 47 | loss: 0.0952068\n",
      "\tspeed: 0.0737s/iter; left time: 3010.0512s\n",
      "\titers: 500, epoch: 47 | loss: 0.0767240\n",
      "\tspeed: 0.0780s/iter; left time: 3178.9700s\n",
      "\titers: 600, epoch: 47 | loss: 0.0796197\n",
      "\tspeed: 0.0786s/iter; left time: 3193.6712s\n",
      "\titers: 700, epoch: 47 | loss: 0.1097955\n",
      "\tspeed: 0.0788s/iter; left time: 3196.9556s\n",
      "Epoch: 47 cost time: 59.50817680358887\n",
      "Epoch: 47, Steps: 764 | Train Loss: 0.1018471 Vali Loss: 0.1797407 Test Loss: 0.1022254\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Updating learning rate to 1.4210854715202004e-17\n",
      "\titers: 100, epoch: 48 | loss: 0.0899147\n",
      "\tspeed: 0.2203s/iter; left time: 8899.8718s\n",
      "\titers: 200, epoch: 48 | loss: 0.1071841\n",
      "\tspeed: 0.0799s/iter; left time: 3220.8950s\n",
      "\titers: 300, epoch: 48 | loss: 0.1096711\n",
      "\tspeed: 0.0776s/iter; left time: 3120.7864s\n",
      "\titers: 400, epoch: 48 | loss: 0.0944421\n",
      "\tspeed: 0.0715s/iter; left time: 2868.4565s\n",
      "\titers: 500, epoch: 48 | loss: 0.1497001\n",
      "\tspeed: 0.0805s/iter; left time: 3218.8274s\n",
      "\titers: 600, epoch: 48 | loss: 0.1511246\n",
      "\tspeed: 0.0805s/iter; left time: 3211.5121s\n",
      "\titers: 700, epoch: 48 | loss: 0.0838506\n",
      "\tspeed: 0.0819s/iter; left time: 3257.2843s\n",
      "Epoch: 48 cost time: 60.2547562122345\n",
      "Epoch: 48, Steps: 764 | Train Loss: 0.1020936 Vali Loss: 0.1787294 Test Loss: 0.1039144\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Updating learning rate to 7.105427357601002e-18\n",
      "\titers: 100, epoch: 49 | loss: 0.1020287\n",
      "\tspeed: 0.2218s/iter; left time: 8789.4909s\n",
      "\titers: 200, epoch: 49 | loss: 0.1548285\n",
      "\tspeed: 0.0790s/iter; left time: 3123.6353s\n",
      "\titers: 300, epoch: 49 | loss: 0.0738181\n",
      "\tspeed: 0.0749s/iter; left time: 2952.8345s\n",
      "\titers: 400, epoch: 49 | loss: 0.0683997\n",
      "\tspeed: 0.0741s/iter; left time: 2915.6992s\n",
      "\titers: 500, epoch: 49 | loss: 0.1193746\n",
      "\tspeed: 0.0793s/iter; left time: 3110.2353s\n",
      "\titers: 600, epoch: 49 | loss: 0.0818917\n",
      "\tspeed: 0.0788s/iter; left time: 3081.4424s\n",
      "\titers: 700, epoch: 49 | loss: 0.0865930\n",
      "\tspeed: 0.0797s/iter; left time: 3111.1728s\n",
      "Epoch: 49 cost time: 59.577439308166504\n",
      "Epoch: 49, Steps: 764 | Train Loss: 0.1021555 Vali Loss: 0.1783846 Test Loss: 0.1035034\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Updating learning rate to 3.552713678800501e-18\n",
      "\titers: 100, epoch: 50 | loss: 0.0975880\n",
      "\tspeed: 0.2210s/iter; left time: 8588.8679s\n",
      "\titers: 200, epoch: 50 | loss: 0.0750005\n",
      "\tspeed: 0.0780s/iter; left time: 3025.3471s\n",
      "\titers: 300, epoch: 50 | loss: 0.0792299\n",
      "\tspeed: 0.0732s/iter; left time: 2828.7909s\n",
      "\titers: 400, epoch: 50 | loss: 0.0937503\n",
      "\tspeed: 0.0749s/iter; left time: 2887.5374s\n",
      "\titers: 500, epoch: 50 | loss: 0.0869058\n",
      "\tspeed: 0.0782s/iter; left time: 3006.7242s\n",
      "\titers: 600, epoch: 50 | loss: 0.1105006\n",
      "\tspeed: 0.0783s/iter; left time: 3005.5057s\n",
      "\titers: 700, epoch: 50 | loss: 0.0935450\n",
      "\tspeed: 0.0789s/iter; left time: 3018.4457s\n",
      "Epoch: 50 cost time: 59.055323362350464\n",
      "Epoch: 50, Steps: 764 | Train Loss: 0.1021562 Vali Loss: 0.1787589 Test Loss: 0.1031917\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Updating learning rate to 1.7763568394002505e-18\n",
      "\titers: 100, epoch: 51 | loss: 0.0949082\n",
      "\tspeed: 0.2235s/iter; left time: 8514.5705s\n",
      "\titers: 200, epoch: 51 | loss: 0.1137600\n",
      "\tspeed: 0.0795s/iter; left time: 3021.1750s\n",
      "\titers: 300, epoch: 51 | loss: 0.0906867\n",
      "\tspeed: 0.0720s/iter; left time: 2727.4562s\n",
      "\titers: 400, epoch: 51 | loss: 0.0967292\n",
      "\tspeed: 0.0765s/iter; left time: 2892.6570s\n",
      "\titers: 500, epoch: 51 | loss: 0.0827238\n",
      "\tspeed: 0.0782s/iter; left time: 2946.5680s\n",
      "\titers: 600, epoch: 51 | loss: 0.1032065\n",
      "\tspeed: 0.0802s/iter; left time: 3015.8227s\n",
      "\titers: 700, epoch: 51 | loss: 0.1082404\n",
      "\tspeed: 0.0797s/iter; left time: 2988.7918s\n",
      "Epoch: 51 cost time: 59.66559553146362\n",
      "\titers: 100, epoch: 53 | loss: 0.0820182\n",
      "\tspeed: 0.2204s/iter; left time: 8062.0211s\n",
      "\titers: 200, epoch: 53 | loss: 0.0933813\n",
      "\tspeed: 0.0755s/iter; left time: 2754.9219s\n",
      "\titers: 300, epoch: 53 | loss: 0.1495082\n",
      "\tspeed: 0.0728s/iter; left time: 2647.0993s\n",
      "\titers: 400, epoch: 53 | loss: 0.1002296\n",
      "\tspeed: 0.0786s/iter; left time: 2850.0511s\n",
      "\titers: 500, epoch: 53 | loss: 0.1429917\n",
      "\tspeed: 0.0800s/iter; left time: 2894.1607s\n",
      "\titers: 600, epoch: 53 | loss: 0.0911243\n",
      "\tspeed: 0.0806s/iter; left time: 2906.2930s\n",
      "\titers: 700, epoch: 53 | loss: 0.1637717\n",
      "\tspeed: 0.0798s/iter; left time: 2871.3317s\n",
      "Epoch: 53 cost time: 59.61704444885254\n",
      "Epoch: 53, Steps: 764 | Train Loss: 0.1017441 Vali Loss: 0.1788541 Test Loss: 0.1024510\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Updating learning rate to 2.220446049250313e-19\n",
      "\titers: 100, epoch: 54 | loss: 0.0775906\n",
      "\tspeed: 0.2230s/iter; left time: 7985.0295s\n",
      "\titers: 200, epoch: 54 | loss: 0.1042092\n",
      "\tspeed: 0.0738s/iter; left time: 2633.7692s\n",
      "\titers: 300, epoch: 54 | loss: 0.1043951\n",
      "\tspeed: 0.0752s/iter; left time: 2678.7821s\n",
      "\titers: 400, epoch: 54 | loss: 0.1137668\n",
      "\tspeed: 0.0799s/iter; left time: 2838.9261s\n",
      "\titers: 500, epoch: 54 | loss: 0.0737837\n",
      "\tspeed: 0.0808s/iter; left time: 2861.2236s\n",
      "\titers: 600, epoch: 54 | loss: 0.0877144\n",
      "\tspeed: 0.0792s/iter; left time: 2796.3901s\n",
      "\titers: 700, epoch: 54 | loss: 0.0808454\n",
      "\tspeed: 0.0806s/iter; left time: 2838.7689s\n",
      "Epoch: 54 cost time: 59.97819638252258\n",
      "Epoch: 54, Steps: 764 | Train Loss: 0.1019712 Vali Loss: 0.1789493 Test Loss: 0.1043574\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Updating learning rate to 1.1102230246251566e-19\n",
      "\titers: 100, epoch: 55 | loss: 0.0819751\n",
      "\tspeed: 0.2231s/iter; left time: 7819.9862s\n",
      "\titers: 200, epoch: 55 | loss: 0.1040236\n",
      "\tspeed: 0.0718s/iter; left time: 2509.4898s\n",
      "\titers: 300, epoch: 55 | loss: 0.0913636\n",
      "\tspeed: 0.0795s/iter; left time: 2770.8179s\n",
      "\titers: 400, epoch: 55 | loss: 0.0886341\n",
      "\tspeed: 0.0792s/iter; left time: 2752.3484s\n",
      "\titers: 500, epoch: 55 | loss: 0.1155384\n",
      "\tspeed: 0.0781s/iter; left time: 2707.1394s\n",
      "\titers: 600, epoch: 55 | loss: 0.0851481\n",
      "\tspeed: 0.0798s/iter; left time: 2757.9050s\n",
      "\titers: 700, epoch: 55 | loss: 0.0848153\n",
      "\tspeed: 0.0800s/iter; left time: 2754.2010s\n",
      "Epoch: 55 cost time: 59.83263850212097\n",
      "Epoch: 55, Steps: 764 | Train Loss: 0.1019542 Vali Loss: 0.1787594 Test Loss: 0.1041507\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Updating learning rate to 5.551115123125783e-20\n",
      "\titers: 100, epoch: 56 | loss: 0.0919473\n",
      "\tspeed: 0.2180s/iter; left time: 7473.1384s\n",
      "\titers: 200, epoch: 56 | loss: 0.1274096\n",
      "\tspeed: 0.0727s/iter; left time: 2484.8808s\n",
      "\titers: 300, epoch: 56 | loss: 0.0955836\n",
      "\tspeed: 0.0785s/iter; left time: 2674.4582s\n",
      "\titers: 400, epoch: 56 | loss: 0.0902017\n",
      "\tspeed: 0.0791s/iter; left time: 2689.5493s\n",
      "\titers: 500, epoch: 56 | loss: 0.1355910\n",
      "\tspeed: 0.0811s/iter; left time: 2747.8711s\n",
      "\titers: 600, epoch: 56 | loss: 0.0931914\n",
      "\tspeed: 0.0787s/iter; left time: 2658.4628s\n",
      "\titers: 700, epoch: 56 | loss: 0.1233631\n",
      "\tspeed: 0.0798s/iter; left time: 2687.2409s\n",
      "Epoch: 56 cost time: 59.51176142692566\n",
      "Epoch: 56, Steps: 764 | Train Loss: 0.1020016 Vali Loss: 0.1788068 Test Loss: 0.1039184\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Updating learning rate to 2.7755575615628914e-20\n",
      "\titers: 100, epoch: 57 | loss: 0.1001924\n",
      "\tspeed: 0.2143s/iter; left time: 7181.0696s\n",
      "\titers: 200, epoch: 57 | loss: 0.0888603\n",
      "\tspeed: 0.0767s/iter; left time: 2561.4194s\n",
      "\titers: 300, epoch: 57 | loss: 0.1027896\n",
      "\tspeed: 0.0789s/iter; left time: 2629.2717s\n",
      "\titers: 400, epoch: 57 | loss: 0.1159295\n",
      "\tspeed: 0.0789s/iter; left time: 2621.1844s\n",
      "\titers: 500, epoch: 57 | loss: 0.0805301\n",
      "\tspeed: 0.0789s/iter; left time: 2612.4374s\n",
      "\titers: 600, epoch: 57 | loss: 0.0915341\n",
      "\tspeed: 0.0778s/iter; left time: 2567.1411s\n",
      "\titers: 700, epoch: 57 | loss: 0.0836364\n",
      "\tspeed: 0.0785s/iter; left time: 2583.4289s\n",
      "Epoch: 57 cost time: 59.39181327819824\n",
      "Epoch: 57, Steps: 764 | Train Loss: 0.1019028 Vali Loss: 0.1792833 Test Loss: 0.1029796\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Updating learning rate to 1.3877787807814457e-20\n",
      "\titers: 100, epoch: 58 | loss: 0.0815764\n",
      "\tspeed: 0.2177s/iter; left time: 7130.8451s\n",
      "\titers: 200, epoch: 58 | loss: 0.0807856\n",
      "\tspeed: 0.0776s/iter; left time: 2535.0937s\n",
      "\titers: 300, epoch: 58 | loss: 0.1276427\n",
      "\tspeed: 0.0781s/iter; left time: 2541.2485s\n",
      "\titers: 400, epoch: 58 | loss: 0.1194990\n",
      "\tspeed: 0.0775s/iter; left time: 2515.8299s\n",
      "\titers: 500, epoch: 58 | loss: 0.0901777\n",
      "\tspeed: 0.0781s/iter; left time: 2525.6621s\n",
      "\titers: 600, epoch: 58 | loss: 0.1432393\n",
      "\tspeed: 0.0796s/iter; left time: 2567.8885s\n",
      "\titers: 700, epoch: 58 | loss: 0.0915024\n",
      "\tspeed: 0.0800s/iter; left time: 2573.1066s\n",
      "Epoch: 58 cost time: 59.4395489692688\n",
      "Epoch: 58, Steps: 764 | Train Loss: 0.1018180 Vali Loss: 0.1795743 Test Loss: 0.1028483\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Updating learning rate to 6.938893903907229e-21\n",
      "\titers: 100, epoch: 59 | loss: 0.1299998\n",
      "\tspeed: 0.2129s/iter; left time: 6810.1563s\n",
      "\titers: 200, epoch: 59 | loss: 0.1119332\n",
      "\tspeed: 0.0793s/iter; left time: 2530.0264s\n",
      "\titers: 300, epoch: 59 | loss: 0.1300868\n",
      "\tspeed: 0.0811s/iter; left time: 2577.9326s\n",
      "\titers: 400, epoch: 59 | loss: 0.1270999\n",
      "\tspeed: 0.0806s/iter; left time: 2552.7502s\n",
      "\titers: 500, epoch: 59 | loss: 0.1153585\n",
      "\tspeed: 0.0796s/iter; left time: 2513.9697s\n",
      "\titers: 600, epoch: 59 | loss: 0.1113782\n",
      "\tspeed: 0.0787s/iter; left time: 2479.1030s\n",
      "\titers: 700, epoch: 59 | loss: 0.0995999\n",
      "\tspeed: 0.0785s/iter; left time: 2465.1271s\n",
      "Epoch: 59 cost time: 60.02606987953186\n",
      "Epoch: 59, Steps: 764 | Train Loss: 0.1019747 Vali Loss: 0.1769255 Test Loss: 0.1034127\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Updating learning rate to 3.469446951953614e-21\n",
      "\titers: 100, epoch: 60 | loss: 0.1008201\n",
      "\tspeed: 0.2130s/iter; left time: 6651.8427s\n",
      "\titers: 200, epoch: 60 | loss: 0.0896634\n",
      "\tspeed: 0.0790s/iter; left time: 2459.1089s\n",
      "\titers: 300, epoch: 60 | loss: 0.1156161\n",
      "\tspeed: 0.0792s/iter; left time: 2457.8523s\n",
      "\titers: 400, epoch: 60 | loss: 0.1223312\n",
      "\tspeed: 0.0799s/iter; left time: 2471.6091s\n",
      "\titers: 500, epoch: 60 | loss: 0.1115365\n",
      "\tspeed: 0.0790s/iter; left time: 2436.5623s\n",
      "\titers: 600, epoch: 60 | loss: 0.0925991\n",
      "\tspeed: 0.0780s/iter; left time: 2395.0849s\n",
      "\titers: 700, epoch: 60 | loss: 0.0826461\n",
      "\tspeed: 0.0785s/iter; left time: 2403.3953s\n",
      "Epoch: 60 cost time: 59.89585280418396\n",
      "Epoch: 60, Steps: 764 | Train Loss: 0.1018729 Vali Loss: 0.1784772 Test Loss: 0.1029648\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Updating learning rate to 1.734723475976807e-21\n",
      "\titers: 100, epoch: 61 | loss: 0.0963462\n",
      "\tspeed: 0.2124s/iter; left time: 6470.6962s\n",
      "\titers: 200, epoch: 61 | loss: 0.0995510\n",
      "\tspeed: 0.0793s/iter; left time: 2408.1083s\n",
      "\titers: 300, epoch: 61 | loss: 0.0905243\n",
      "\tspeed: 0.0814s/iter; left time: 2464.3423s\n",
      "\titers: 400, epoch: 61 | loss: 0.1249144\n",
      "\tspeed: 0.0796s/iter; left time: 2402.0079s\n",
      "\titers: 500, epoch: 61 | loss: 0.1240694\n",
      "\tspeed: 0.0785s/iter; left time: 2359.2383s\n",
      "\titers: 600, epoch: 61 | loss: 0.1170574\n",
      "\tspeed: 0.0783s/iter; left time: 2346.9690s\n",
      "\titers: 700, epoch: 61 | loss: 0.0916979\n",
      "\tspeed: 0.0789s/iter; left time: 2354.7158s\n",
      "Epoch: 61 cost time: 60.108171463012695\n",
      "Epoch: 61, Steps: 764 | Train Loss: 0.1021201 Vali Loss: 0.1782017 Test Loss: 0.1042661\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Updating learning rate to 8.673617379884036e-22\n",
      "\titers: 100, epoch: 62 | loss: 0.0720126\n",
      "\tspeed: 0.2101s/iter; left time: 6240.2430s\n",
      "\titers: 200, epoch: 62 | loss: 0.1137342\n",
      "\tspeed: 0.0814s/iter; left time: 2409.9598s\n",
      "\titers: 300, epoch: 62 | loss: 0.1022865\n",
      "\tspeed: 0.0816s/iter; left time: 2405.7932s\n",
      "\titers: 400, epoch: 62 | loss: 0.0756766\n",
      "\tspeed: 0.0803s/iter; left time: 2361.9038s\n",
      "\titers: 500, epoch: 62 | loss: 0.0843150\n",
      "\tspeed: 0.0801s/iter; left time: 2345.3999s\n",
      "\titers: 600, epoch: 62 | loss: 0.0941854\n",
      "\tspeed: 0.0802s/iter; left time: 2342.0634s\n",
      "\titers: 700, epoch: 62 | loss: 0.1145236\n",
      "\tspeed: 0.0792s/iter; left time: 2303.2404s\n",
      "Epoch: 62 cost time: 60.960578203201294\n",
      "Epoch: 62, Steps: 764 | Train Loss: 0.1021859 Vali Loss: 0.1776476 Test Loss: 0.1029944\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Updating learning rate to 4.336808689942018e-22\n",
      "\titers: 100, epoch: 63 | loss: 0.1041074\n",
      "\tspeed: 0.2101s/iter; left time: 6079.2824s\n",
      "\titers: 200, epoch: 63 | loss: 0.0906787\n",
      "\tspeed: 0.0780s/iter; left time: 2249.3486s\n",
      "\titers: 300, epoch: 63 | loss: 0.0761427\n",
      "\tspeed: 0.0779s/iter; left time: 2237.5128s\n",
      "\titers: 500, epoch: 63 | loss: 0.0826313\n",
      "\tspeed: 0.0811s/iter; left time: 2314.7037s\n",
      "\titers: 600, epoch: 63 | loss: 0.1024473\n",
      "\tspeed: 0.0791s/iter; left time: 2247.8147s\n",
      "\titers: 700, epoch: 63 | loss: 0.0771172\n",
      "\tspeed: 0.0789s/iter; left time: 2234.2488s\n",
      "Epoch: 63 cost time: 60.481107234954834\n",
      "Epoch: 63, Steps: 764 | Train Loss: 0.1020085 Vali Loss: 0.1784035 Test Loss: 0.1041219\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Updating learning rate to 2.168404344971009e-22\n",
      "\titers: 100, epoch: 64 | loss: 0.1234625\n",
      "\tspeed: 0.2123s/iter; left time: 5980.7243s\n",
      "\titers: 200, epoch: 64 | loss: 0.0900374\n",
      "\tspeed: 0.0781s/iter; left time: 2193.2887s\n",
      "\titers: 300, epoch: 64 | loss: 0.0830875\n",
      "\tspeed: 0.0799s/iter; left time: 2235.1439s\n",
      "\titers: 400, epoch: 64 | loss: 0.1268422\n",
      "\tspeed: 0.0792s/iter; left time: 2208.1270s\n",
      "\titers: 500, epoch: 64 | loss: 0.0851724\n",
      "\tspeed: 0.0784s/iter; left time: 2176.7336s\n",
      "\titers: 600, epoch: 64 | loss: 0.1074273\n",
      "\tspeed: 0.0782s/iter; left time: 2163.3179s\n",
      "\titers: 700, epoch: 64 | loss: 0.1109484\n",
      "\tspeed: 0.0790s/iter; left time: 2178.2803s\n",
      "Epoch: 64 cost time: 60.031073570251465\n",
      "Epoch: 64, Steps: 764 | Train Loss: 0.1019482 Vali Loss: 0.1796471 Test Loss: 0.1033619\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Updating learning rate to 1.0842021724855045e-22\n",
      "\titers: 100, epoch: 65 | loss: 0.0842377\n",
      "\tspeed: 0.2126s/iter; left time: 5825.6823s\n",
      "\titers: 200, epoch: 65 | loss: 0.1366292\n",
      "\tspeed: 0.0795s/iter; left time: 2170.3618s\n",
      "\titers: 300, epoch: 65 | loss: 0.1015487\n",
      "\tspeed: 0.0777s/iter; left time: 2113.6106s\n",
      "\titers: 400, epoch: 65 | loss: 0.1101051\n",
      "\tspeed: 0.0774s/iter; left time: 2098.9435s\n",
      "\titers: 500, epoch: 65 | loss: 0.1134881\n",
      "\tspeed: 0.0792s/iter; left time: 2137.7998s\n",
      "\titers: 600, epoch: 65 | loss: 0.0787972\n",
      "\tspeed: 0.0800s/iter; left time: 2152.0353s\n",
      "\titers: 700, epoch: 65 | loss: 0.0830152\n",
      "\tspeed: 0.0799s/iter; left time: 2141.4463s\n",
      "Epoch: 65 cost time: 59.95194697380066\n",
      "Epoch: 65, Steps: 764 | Train Loss: 0.1019659 Vali Loss: 0.1795861 Test Loss: 0.1027137\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Updating learning rate to 5.421010862427522e-23\n",
      "\titers: 100, epoch: 66 | loss: 0.0871432\n",
      "\tspeed: 0.2140s/iter; left time: 5699.9934s\n",
      "\titers: 200, epoch: 66 | loss: 0.1118408\n",
      "\tspeed: 0.0798s/iter; left time: 2118.0645s\n",
      "\titers: 300, epoch: 66 | loss: 0.0817377\n",
      "\tspeed: 0.0787s/iter; left time: 2081.7814s\n",
      "\titers: 400, epoch: 66 | loss: 0.0951891\n",
      "\tspeed: 0.0784s/iter; left time: 2064.3377s\n",
      "\titers: 500, epoch: 66 | loss: 0.0974943\n",
      "\tspeed: 0.0784s/iter; left time: 2056.6065s\n",
      "\titers: 600, epoch: 66 | loss: 0.1162596\n",
      "\tspeed: 0.0788s/iter; left time: 2060.1814s\n",
      "\titers: 700, epoch: 66 | loss: 0.1145901\n",
      "\tspeed: 0.0780s/iter; left time: 2031.7063s\n",
      "Epoch: 66 cost time: 59.83974289894104\n",
      "Epoch: 66, Steps: 764 | Train Loss: 0.1019818 Vali Loss: 0.1783239 Test Loss: 0.1046449\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Updating learning rate to 2.710505431213761e-23\n",
      "\titers: 100, epoch: 67 | loss: 0.0722863\n",
      "\tspeed: 0.2128s/iter; left time: 5506.7452s\n",
      "\titers: 200, epoch: 67 | loss: 0.1092600\n",
      "\tspeed: 0.0793s/iter; left time: 2043.5054s\n",
      "\titers: 300, epoch: 67 | loss: 0.0857006\n",
      "\tspeed: 0.0788s/iter; left time: 2022.5705s\n",
      "\titers: 400, epoch: 67 | loss: 0.0914343\n",
      "\tspeed: 0.0802s/iter; left time: 2050.9638s\n",
      "\titers: 500, epoch: 67 | loss: 0.1181867\n",
      "\tspeed: 0.0787s/iter; left time: 2004.6924s\n",
      "\titers: 600, epoch: 67 | loss: 0.0755498\n",
      "\tspeed: 0.0782s/iter; left time: 1985.0501s\n",
      "\titers: 700, epoch: 67 | loss: 0.0851793\n",
      "\tspeed: 0.0766s/iter; left time: 1934.9786s\n",
      "Epoch: 67 cost time: 59.808242321014404\n",
      "Epoch: 67, Steps: 764 | Train Loss: 0.1021230 Vali Loss: 0.1780173 Test Loss: 0.1029025\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Updating learning rate to 1.3552527156068806e-23\n",
      "\titers: 100, epoch: 68 | loss: 0.1125147\n",
      "\tspeed: 0.2186s/iter; left time: 5489.5912s\n",
      "\titers: 200, epoch: 68 | loss: 0.0768714\n",
      "\tspeed: 0.0793s/iter; left time: 1982.2973s\n",
      "\titers: 300, epoch: 68 | loss: 0.0823573\n",
      "\tspeed: 0.0790s/iter; left time: 1969.1734s\n",
      "\titers: 400, epoch: 68 | loss: 0.0976759\n",
      "\tspeed: 0.0802s/iter; left time: 1989.2422s\n",
      "\titers: 500, epoch: 68 | loss: 0.0824410\n",
      "\tspeed: 0.0803s/iter; left time: 1985.2554s\n",
      "\titers: 600, epoch: 68 | loss: 0.1062293\n",
      "\tspeed: 0.0795s/iter; left time: 1955.6232s\n",
      "\titers: 700, epoch: 68 | loss: 0.1161583\n",
      "\tspeed: 0.0759s/iter; left time: 1861.5399s\n",
      "Epoch: 68 cost time: 60.2612566947937\n",
      "Epoch: 68, Steps: 764 | Train Loss: 0.1020764 Vali Loss: 0.1778102 Test Loss: 0.1040772\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Updating learning rate to 6.776263578034403e-24\n",
      "\titers: 100, epoch: 69 | loss: 0.0756019\n",
      "\tspeed: 0.2193s/iter; left time: 5339.6774s\n",
      "\titers: 200, epoch: 69 | loss: 0.0900361\n",
      "\tspeed: 0.0799s/iter; left time: 1938.4780s\n",
      "\titers: 300, epoch: 69 | loss: 0.1221877\n",
      "\tspeed: 0.0814s/iter; left time: 1965.7150s\n",
      "\titers: 400, epoch: 69 | loss: 0.0940350\n",
      "\tspeed: 0.0808s/iter; left time: 1944.1958s\n",
      "\titers: 500, epoch: 69 | loss: 0.1039147\n",
      "\tspeed: 0.0814s/iter; left time: 1949.5455s\n",
      "\titers: 600, epoch: 69 | loss: 0.1025564\n",
      "\tspeed: 0.0783s/iter; left time: 1868.2930s\n",
      "\titers: 700, epoch: 69 | loss: 0.1118456\n",
      "\tspeed: 0.0723s/iter; left time: 1716.5872s\n",
      "Epoch: 69 cost time: 60.27612805366516\n",
      "Epoch: 69, Steps: 764 | Train Loss: 0.1017029 Vali Loss: 0.1783614 Test Loss: 0.1033653\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Updating learning rate to 3.3881317890172014e-24\n",
      "\titers: 100, epoch: 70 | loss: 0.0959939\n",
      "\tspeed: 0.2212s/iter; left time: 5216.3101s\n",
      "\titers: 200, epoch: 70 | loss: 0.1012405\n",
      "\tspeed: 0.0791s/iter; left time: 1857.1886s\n",
      "\titers: 300, epoch: 70 | loss: 0.0719105\n",
      "\tspeed: 0.0791s/iter; left time: 1850.5815s\n",
      "\titers: 400, epoch: 70 | loss: 0.0908281\n",
      "\tspeed: 0.0799s/iter; left time: 1861.2969s\n",
      "\titers: 500, epoch: 70 | loss: 0.0924327\n",
      "\tspeed: 0.0797s/iter; left time: 1848.3415s\n",
      "\titers: 600, epoch: 70 | loss: 0.0912204\n",
      "\tspeed: 0.0744s/iter; left time: 1718.2286s\n",
      "\titers: 700, epoch: 70 | loss: 0.1254093\n",
      "\tspeed: 0.0733s/iter; left time: 1684.1807s\n",
      "Epoch: 70 cost time: 59.71140909194946\n",
      "Epoch: 70, Steps: 764 | Train Loss: 0.1018862 Vali Loss: 0.1787596 Test Loss: 0.1037541\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Updating learning rate to 1.6940658945086007e-24\n",
      "\titers: 100, epoch: 71 | loss: 0.1122295\n",
      "\tspeed: 0.2213s/iter; left time: 5050.7451s\n",
      "\titers: 200, epoch: 71 | loss: 0.0857405\n",
      "\tspeed: 0.0785s/iter; left time: 1784.5824s\n",
      "\titers: 300, epoch: 71 | loss: 0.0917487\n",
      "\tspeed: 0.0791s/iter; left time: 1789.0680s\n",
      "\titers: 400, epoch: 71 | loss: 0.0634012\n",
      "\tspeed: 0.0795s/iter; left time: 1789.9875s\n",
      "\titers: 500, epoch: 71 | loss: 0.0742029\n",
      "\tspeed: 0.0823s/iter; left time: 1845.6902s\n",
      "\titers: 600, epoch: 71 | loss: 0.1118314\n",
      "\tspeed: 0.0753s/iter; left time: 1680.9101s\n",
      "\titers: 700, epoch: 71 | loss: 0.1358992\n",
      "\tspeed: 0.0764s/iter; left time: 1698.0193s\n",
      "Epoch: 71 cost time: 60.15594005584717\n",
      "Epoch: 71, Steps: 764 | Train Loss: 0.1019500 Vali Loss: 0.1775588 Test Loss: 0.1037779\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Updating learning rate to 8.470329472543004e-25\n",
      "\titers: 100, epoch: 72 | loss: 0.1105443\n",
      "\tspeed: 0.2238s/iter; left time: 4935.4909s\n",
      "\titers: 200, epoch: 72 | loss: 0.0924226\n",
      "\tspeed: 0.0803s/iter; left time: 1764.0820s\n",
      "\titers: 300, epoch: 72 | loss: 0.1358925\n",
      "\tspeed: 0.0785s/iter; left time: 1715.6418s\n",
      "\titers: 400, epoch: 72 | loss: 0.0988494\n",
      "\tspeed: 0.0783s/iter; left time: 1703.4078s\n",
      "\titers: 500, epoch: 72 | loss: 0.0727756\n",
      "\tspeed: 0.0773s/iter; left time: 1674.6071s\n",
      "\titers: 600, epoch: 72 | loss: 0.1380806\n",
      "\tspeed: 0.0734s/iter; left time: 1581.6213s\n",
      "\titers: 700, epoch: 72 | loss: 0.0957593\n",
      "\tspeed: 0.0784s/iter; left time: 1683.2797s\n",
      "Epoch: 72 cost time: 59.67206048965454\n",
      "Epoch: 72, Steps: 764 | Train Loss: 0.1018569 Vali Loss: 0.1788284 Test Loss: 0.1033308\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Updating learning rate to 4.235164736271502e-25\n",
      "\titers: 100, epoch: 73 | loss: 0.0999181\n",
      "\tspeed: 0.2243s/iter; left time: 4775.6569s\n",
      "\titers: 200, epoch: 73 | loss: 0.0821017\n",
      "\tspeed: 0.0809s/iter; left time: 1714.9112s\n",
      "\titers: 300, epoch: 73 | loss: 0.0979370\n",
      "\tspeed: 0.0791s/iter; left time: 1668.8719s\n",
      "\titers: 400, epoch: 73 | loss: 0.0751221\n",
      "\tspeed: 0.0792s/iter; left time: 1663.0042s\n",
      "\titers: 500, epoch: 73 | loss: 0.1126844\n",
      "\tspeed: 0.0776s/iter; left time: 1621.4303s\n",
      "\titers: 600, epoch: 73 | loss: 0.0939341\n",
      "\tspeed: 0.0743s/iter; left time: 1544.0267s\n",
      "\titers: 700, epoch: 73 | loss: 0.0988878\n",
      "\tspeed: 0.0785s/iter; left time: 1624.8060s\n",
      "Epoch: 73 cost time: 60.162235736846924\n",
      "Epoch: 73, Steps: 764 | Train Loss: 0.1019452 Vali Loss: 0.1782810 Test Loss: 0.1043303\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Updating learning rate to 2.117582368135751e-25\n",
      "\titers: 100, epoch: 74 | loss: 0.0903235\n",
      "\tspeed: 0.2255s/iter; left time: 4629.6520s\n",
      "\titers: 200, epoch: 74 | loss: 0.1222175\n",
      "\tspeed: 0.0815s/iter; left time: 1665.3947s\n",
      "\titers: 300, epoch: 74 | loss: 0.1025997\n",
      "\tspeed: 0.0780s/iter; left time: 1586.5399s\n",
      "\titers: 400, epoch: 74 | loss: 0.0858944\n",
      "\tspeed: 0.0796s/iter; left time: 1609.7244s\n",
      "\titers: 500, epoch: 74 | loss: 0.0760590\n",
      "\tspeed: 0.0722s/iter; left time: 1453.4804s\n",
      "\titers: 600, epoch: 74 | loss: 0.1240488\n",
      "\tspeed: 0.0773s/iter; left time: 1547.6832s\n",
      "\titers: 700, epoch: 74 | loss: 0.1309675\n",
      "\tspeed: 0.0791s/iter; left time: 1575.6024s\n",
      "Epoch: 74 cost time: 60.03695774078369\n",
      "Epoch: 74, Steps: 764 | Train Loss: 0.1019422 Vali Loss: 0.1785847 Test Loss: 0.1036428\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Updating learning rate to 1.0587911840678754e-25\n",
      "\titers: 100, epoch: 75 | loss: 0.0767882\n",
      "\tspeed: 0.2239s/iter; left time: 4425.6347s\n",
      "\titers: 200, epoch: 75 | loss: 0.0746194\n",
      "\tspeed: 0.0785s/iter; left time: 1544.5613s\n",
      "\titers: 300, epoch: 75 | loss: 0.0982980\n",
      "\tspeed: 0.0788s/iter; left time: 1541.5200s\n",
      "\titers: 400, epoch: 75 | loss: 0.1192516\n",
      "\tspeed: 0.0769s/iter; left time: 1497.0589s\n",
      "\titers: 500, epoch: 75 | loss: 0.1048704\n",
      "\tspeed: 0.0716s/iter; left time: 1385.7490s\n",
      "\titers: 600, epoch: 75 | loss: 0.1000996\n",
      "\tspeed: 0.0785s/iter; left time: 1511.7748s\n",
      "\titers: 700, epoch: 75 | loss: 0.1090916\n",
      "\tspeed: 0.0794s/iter; left time: 1521.1208s\n",
      "Epoch: 75 cost time: 59.29844689369202\n",
      "Epoch: 75, Steps: 764 | Train Loss: 0.1018470 Vali Loss: 0.1789931 Test Loss: 0.1038240\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Updating learning rate to 5.293955920339377e-26\n",
      "\titers: 100, epoch: 76 | loss: 0.1177377\n",
      "\tspeed: 0.2227s/iter; left time: 4230.9000s\n",
      "\titers: 200, epoch: 76 | loss: 0.0928981\n",
      "\tspeed: 0.0786s/iter; left time: 1486.2899s\n",
      "\titers: 300, epoch: 76 | loss: 0.0799241\n",
      "\tspeed: 0.0786s/iter; left time: 1478.5140s\n",
      "\titers: 400, epoch: 76 | loss: 0.0871468\n",
      "\tspeed: 0.0761s/iter; left time: 1423.5298s\n",
      "\titers: 500, epoch: 76 | loss: 0.1020985\n",
      "\tspeed: 0.0724s/iter; left time: 1346.8258s\n",
      "\titers: 600, epoch: 76 | loss: 0.1174810\n",
      "\tspeed: 0.0784s/iter; left time: 1449.5660s\n",
      "\titers: 700, epoch: 76 | loss: 0.0943663\n",
      "\tspeed: 0.0790s/iter; left time: 1454.0086s\n",
      "Epoch: 76 cost time: 59.36644792556763\n",
      "Epoch: 76, Steps: 764 | Train Loss: 0.1020911 Vali Loss: 0.1781847 Test Loss: 0.1036025\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Updating learning rate to 2.6469779601696886e-26\n",
      "\titers: 100, epoch: 77 | loss: 0.1210195\n",
      "\tspeed: 0.2217s/iter; left time: 4043.2577s\n",
      "\titers: 200, epoch: 77 | loss: 0.1355938\n",
      "\tspeed: 0.0815s/iter; left time: 1478.8583s\n",
      "\titers: 300, epoch: 77 | loss: 0.0820748\n",
      "\tspeed: 0.0804s/iter; left time: 1449.3086s\n",
      "\titers: 400, epoch: 77 | loss: 0.1042942\n",
      "\tspeed: 0.0748s/iter; left time: 1341.9036s\n",
      "\titers: 500, epoch: 77 | loss: 0.0929243\n",
      "\tspeed: 0.0763s/iter; left time: 1360.4545s\n",
      "\titers: 600, epoch: 77 | loss: 0.1071559\n",
      "\tspeed: 0.0798s/iter; left time: 1416.1217s\n",
      "\titers: 700, epoch: 77 | loss: 0.1373726\n",
      "\tspeed: 0.0797s/iter; left time: 1405.3074s\n",
      "Epoch: 77 cost time: 60.18544888496399\n",
      "Epoch: 77, Steps: 764 | Train Loss: 0.1019072 Vali Loss: 0.1787191 Test Loss: 0.1031155\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Updating learning rate to 1.3234889800848443e-26\n",
      "\titers: 100, epoch: 78 | loss: 0.1127912\n",
      "\tspeed: 0.2235s/iter; left time: 3904.5398s\n",
      "\titers: 200, epoch: 78 | loss: 0.0774358\n",
      "\tspeed: 0.0787s/iter; left time: 1367.1871s\n",
      "\titers: 300, epoch: 78 | loss: 0.0932838\n",
      "\tspeed: 0.0781s/iter; left time: 1349.2622s\n",
      "\titers: 400, epoch: 78 | loss: 0.1056462\n",
      "\tspeed: 0.0711s/iter; left time: 1221.8318s\n",
      "\titers: 500, epoch: 78 | loss: 0.1185578\n",
      "\tspeed: 0.0778s/iter; left time: 1328.4732s\n",
      "\titers: 600, epoch: 78 | loss: 0.1251021\n",
      "\tspeed: 0.0791s/iter; left time: 1342.8137s\n",
      "\titers: 700, epoch: 78 | loss: 0.1314088\n",
      "\tspeed: 0.0784s/iter; left time: 1322.7600s\n",
      "Epoch: 78 cost time: 59.28746700286865\n",
      "Epoch: 78, Steps: 764 | Train Loss: 0.1017573 Vali Loss: 0.1801950 Test Loss: 0.1040612\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Updating learning rate to 6.617444900424222e-27\n",
      "\titers: 100, epoch: 79 | loss: 0.0800360\n",
      "\tspeed: 0.2214s/iter; left time: 3699.7371s\n",
      "\titers: 200, epoch: 79 | loss: 0.1061016\n",
      "\tspeed: 0.0789s/iter; left time: 1310.5063s\n",
      "\titers: 300, epoch: 79 | loss: 0.0915620\n",
      "\tspeed: 0.0766s/iter; left time: 1264.4286s\n",
      "\titers: 400, epoch: 79 | loss: 0.1114891\n",
      "\tspeed: 0.0722s/iter; left time: 1184.5153s\n",
      "\titers: 500, epoch: 79 | loss: 0.0843401\n",
      "\tspeed: 0.0789s/iter; left time: 1286.1999s\n",
      "\titers: 600, epoch: 79 | loss: 0.1269436\n",
      "\tspeed: 0.0789s/iter; left time: 1278.3121s\n",
      "\titers: 700, epoch: 79 | loss: 0.0900295\n",
      "\tspeed: 0.0802s/iter; left time: 1291.6158s\n",
      "Epoch: 79 cost time: 59.55488157272339\n",
      "Epoch: 79, Steps: 764 | Train Loss: 0.1019111 Vali Loss: 0.1788340 Test Loss: 0.1036962\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Updating learning rate to 3.308722450212111e-27\n",
      "\titers: 100, epoch: 80 | loss: 0.1250709\n",
      "\tspeed: 0.2230s/iter; left time: 3555.8389s\n",
      "\titers: 200, epoch: 80 | loss: 0.1406775\n",
      "\tspeed: 0.0813s/iter; left time: 1288.1932s\n",
      "\titers: 300, epoch: 80 | loss: 0.0903644\n",
      "\tspeed: 0.0756s/iter; left time: 1190.6088s\n",
      "\titers: 400, epoch: 80 | loss: 0.0786649\n",
      "\tspeed: 0.0745s/iter; left time: 1165.1334s\n",
      "\titers: 500, epoch: 80 | loss: 0.1163733\n",
      "\tspeed: 0.0802s/iter; left time: 1245.9464s\n",
      "\titers: 600, epoch: 80 | loss: 0.0809582\n",
      "\tspeed: 0.0787s/iter; left time: 1214.9964s\n",
      "\titers: 700, epoch: 80 | loss: 0.1031644\n",
      "\tspeed: 0.0798s/iter; left time: 1224.6743s\n",
      "Epoch: 80 cost time: 59.9536657333374\n",
      "Epoch: 80, Steps: 764 | Train Loss: 0.1022304 Vali Loss: 0.1782062 Test Loss: 0.1038857\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Updating learning rate to 1.6543612251060554e-27\n",
      "\titers: 100, epoch: 81 | loss: 0.1661240\n",
      "\tspeed: 0.2235s/iter; left time: 3393.6207s\n",
      "\titers: 200, epoch: 81 | loss: 0.0962121\n",
      "\tspeed: 0.0783s/iter; left time: 1180.7193s\n",
      "\titers: 300, epoch: 81 | loss: 0.1070170\n",
      "\tspeed: 0.0737s/iter; left time: 1103.6840s\n",
      "\titers: 400, epoch: 81 | loss: 0.0957518\n",
      "\tspeed: 0.0811s/iter; left time: 1206.8912s\n",
      "\titers: 500, epoch: 81 | loss: 0.0789545\n",
      "\tspeed: 0.0832s/iter; left time: 1230.0556s\n",
      "\titers: 600, epoch: 81 | loss: 0.1142464\n",
      "\tspeed: 0.0813s/iter; left time: 1193.2972s\n",
      "\titers: 700, epoch: 81 | loss: 0.1592221\n",
      "\tspeed: 0.0803s/iter; left time: 1171.2161s\n",
      "Epoch: 81 cost time: 60.83801245689392\n",
      "Epoch: 81, Steps: 764 | Train Loss: 0.1021727 Vali Loss: 0.1797048 Test Loss: 0.1025203\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Updating learning rate to 8.271806125530277e-28\n",
      "\titers: 100, epoch: 82 | loss: 0.0962525\n",
      "\tspeed: 0.2204s/iter; left time: 3178.1463s\n",
      "\titers: 200, epoch: 82 | loss: 0.0822062\n",
      "\tspeed: 0.0784s/iter; left time: 1122.6883s\n",
      "\titers: 300, epoch: 82 | loss: 0.0711857\n",
      "\tspeed: 0.0744s/iter; left time: 1057.6016s\n",
      "\titers: 400, epoch: 82 | loss: 0.0746953\n",
      "\tspeed: 0.0794s/iter; left time: 1121.1106s\n",
      "\titers: 500, epoch: 82 | loss: 0.1000392\n",
      "\tspeed: 0.0788s/iter; left time: 1104.0196s\n",
      "\titers: 600, epoch: 82 | loss: 0.1020543\n",
      "\tspeed: 0.0793s/iter; left time: 1102.9381s\n",
      "\titers: 700, epoch: 82 | loss: 0.1328137\n",
      "\tspeed: 0.0795s/iter; left time: 1097.8585s\n",
      "Epoch: 82 cost time: 60.07103252410889\n",
      "Epoch: 82, Steps: 764 | Train Loss: 0.1021269 Vali Loss: 0.1788854 Test Loss: 0.1033283\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Updating learning rate to 4.1359030627651385e-28\n",
      "\titers: 100, epoch: 83 | loss: 0.1035719\n",
      "\tspeed: 0.2258s/iter; left time: 3082.8939s\n",
      "\titers: 200, epoch: 83 | loss: 0.0903168\n",
      "\tspeed: 0.0729s/iter; left time: 987.5495s\n",
      "\titers: 300, epoch: 83 | loss: 0.0953325\n",
      "\tspeed: 0.0749s/iter; left time: 1007.5020s\n",
      "\titers: 400, epoch: 83 | loss: 0.1246019\n",
      "\tspeed: 0.0781s/iter; left time: 1043.0318s\n",
      "\titers: 500, epoch: 83 | loss: 0.0982319\n",
      "\tspeed: 0.0781s/iter; left time: 1035.3231s\n",
      "\titers: 600, epoch: 83 | loss: 0.0797984\n",
      "\tspeed: 0.0795s/iter; left time: 1045.5818s\n",
      "\titers: 700, epoch: 83 | loss: 0.0903606\n",
      "\tspeed: 0.0782s/iter; left time: 1020.3467s\n",
      "Epoch: 83 cost time: 59.40094757080078\n",
      "Epoch: 83, Steps: 764 | Train Loss: 0.1020883 Vali Loss: 0.1785644 Test Loss: 0.1038049\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Updating learning rate to 2.0679515313825692e-28\n",
      "\titers: 100, epoch: 84 | loss: 0.0927672\n",
      "\tspeed: 0.2236s/iter; left time: 2881.7650s\n",
      "\titers: 200, epoch: 84 | loss: 0.1010679\n",
      "\tspeed: 0.0718s/iter; left time: 918.5573s\n",
      "\titers: 300, epoch: 84 | loss: 0.1081302\n",
      "\tspeed: 0.0776s/iter; left time: 984.0641s\n",
      "\titers: 400, epoch: 84 | loss: 0.0951439\n",
      "\tspeed: 0.0790s/iter; left time: 994.8673s\n",
      "\titers: 500, epoch: 84 | loss: 0.1040638\n",
      "\tspeed: 0.0784s/iter; left time: 979.4525s\n",
      "\titers: 600, epoch: 84 | loss: 0.1385663\n",
      "\tspeed: 0.0790s/iter; left time: 979.2191s\n",
      "\titers: 700, epoch: 84 | loss: 0.0779739\n",
      "\tspeed: 0.0803s/iter; left time: 987.2233s\n",
      "Epoch: 84 cost time: 59.630295276641846\n",
      "Epoch: 84, Steps: 764 | Train Loss: 0.1020844 Vali Loss: 0.1779761 Test Loss: 0.1038201\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Updating learning rate to 1.0339757656912846e-28\n",
      "\titers: 100, epoch: 85 | loss: 0.1052847\n",
      "\tspeed: 0.2203s/iter; left time: 2671.4756s\n",
      "\titers: 200, epoch: 85 | loss: 0.0983514\n",
      "\tspeed: 0.0723s/iter; left time: 869.8476s\n",
      "\titers: 300, epoch: 85 | loss: 0.1117188\n",
      "\tspeed: 0.0800s/iter; left time: 953.8154s\n",
      "\titers: 400, epoch: 85 | loss: 0.1291342\n",
      "\tspeed: 0.0797s/iter; left time: 941.8840s\n",
      "\titers: 500, epoch: 85 | loss: 0.0866254\n",
      "\tspeed: 0.0792s/iter; left time: 928.1338s\n",
      "\titers: 600, epoch: 85 | loss: 0.0930151\n",
      "\tspeed: 0.0788s/iter; left time: 916.0854s\n",
      "\titers: 700, epoch: 85 | loss: 0.1036381\n",
      "\tspeed: 0.0785s/iter; left time: 904.7904s\n",
      "Epoch: 85 cost time: 59.574867486953735\n",
      "Epoch: 85, Steps: 764 | Train Loss: 0.1020484 Vali Loss: 0.1782757 Test Loss: 0.1038899\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Updating learning rate to 5.169878828456423e-29\n",
      "\titers: 100, epoch: 86 | loss: 0.1374095\n",
      "\tspeed: 0.2175s/iter; left time: 2470.7750s\n",
      "\titers: 200, epoch: 86 | loss: 0.0861181\n",
      "\tspeed: 0.0743s/iter; left time: 837.1162s\n",
      "\titers: 300, epoch: 86 | loss: 0.0665975\n",
      "\tspeed: 0.0785s/iter; left time: 875.8632s\n",
      "\titers: 400, epoch: 86 | loss: 0.0751694\n",
      "\tspeed: 0.0809s/iter; left time: 895.0090s\n",
      "\titers: 500, epoch: 86 | loss: 0.0742791\n",
      "\tspeed: 0.0792s/iter; left time: 867.8544s\n",
      "\titers: 600, epoch: 86 | loss: 0.1034959\n",
      "\tspeed: 0.0781s/iter; left time: 848.5402s\n",
      "\titers: 700, epoch: 86 | loss: 0.0979374\n",
      "\tspeed: 0.0797s/iter; left time: 857.6949s\n",
      "Epoch: 86 cost time: 59.956079959869385\n",
      "Epoch: 86, Steps: 764 | Train Loss: 0.1017955 Vali Loss: 0.1782351 Test Loss: 0.1038754\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Updating learning rate to 2.5849394142282115e-29\n",
      "\titers: 100, epoch: 87 | loss: 0.1007749\n",
      "\tspeed: 0.2163s/iter; left time: 2291.7916s\n",
      "\titers: 200, epoch: 87 | loss: 0.1269471\n",
      "\tspeed: 0.0782s/iter; left time: 821.1192s\n",
      "\titers: 300, epoch: 87 | loss: 0.0989463\n",
      "\tspeed: 0.0794s/iter; left time: 825.4891s\n",
      "\titers: 400, epoch: 87 | loss: 0.0749845\n",
      "\tspeed: 0.0785s/iter; left time: 808.4388s\n",
      "\titers: 500, epoch: 87 | loss: 0.0992518\n",
      "\tspeed: 0.0801s/iter; left time: 816.6536s\n",
      "\titers: 600, epoch: 87 | loss: 0.1143047\n",
      "\tspeed: 0.0784s/iter; left time: 791.9165s\n",
      "\titers: 700, epoch: 87 | loss: 0.0747575\n",
      "\tspeed: 0.0802s/iter; left time: 801.7920s\n",
      "Epoch: 87 cost time: 59.783050775527954\n",
      "Epoch: 87, Steps: 764 | Train Loss: 0.1019291 Vali Loss: 0.1786424 Test Loss: 0.1029379\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Updating learning rate to 1.2924697071141058e-29\n",
      "\titers: 100, epoch: 88 | loss: 0.0969637\n",
      "\tspeed: 0.2127s/iter; left time: 2091.4155s\n",
      "\titers: 200, epoch: 88 | loss: 0.1167815\n",
      "\tspeed: 0.0782s/iter; left time: 761.2040s\n",
      "\titers: 300, epoch: 88 | loss: 0.0981578\n",
      "\tspeed: 0.0782s/iter; left time: 753.1631s\n",
      "\titers: 400, epoch: 88 | loss: 0.0862006\n",
      "\tspeed: 0.0790s/iter; left time: 753.3419s\n",
      "\titers: 500, epoch: 88 | loss: 0.0861025\n",
      "\tspeed: 0.0785s/iter; left time: 740.3259s\n",
      "\titers: 600, epoch: 88 | loss: 0.0854778\n",
      "\tspeed: 0.0787s/iter; left time: 734.4694s\n",
      "\titers: 700, epoch: 88 | loss: 0.1641727\n",
      "\tspeed: 0.0777s/iter; left time: 717.3936s\n",
      "Epoch: 88 cost time: 59.13183903694153\n",
      "Epoch: 88, Steps: 764 | Train Loss: 0.1019359 Vali Loss: 0.1779541 Test Loss: 0.1034752\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Updating learning rate to 6.462348535570529e-30\n",
      "\titers: 100, epoch: 89 | loss: 0.1102985\n",
      "\tspeed: 0.2144s/iter; left time: 1944.3328s\n",
      "\titers: 200, epoch: 89 | loss: 0.0941489\n",
      "\tspeed: 0.0784s/iter; left time: 703.4223s\n",
      "\titers: 300, epoch: 89 | loss: 0.0856239\n",
      "\tspeed: 0.0791s/iter; left time: 701.4019s\n",
      "\titers: 400, epoch: 89 | loss: 0.1131525\n",
      "\tspeed: 0.0788s/iter; left time: 690.9550s\n",
      "\titers: 500, epoch: 89 | loss: 0.1339686\n",
      "\tspeed: 0.0807s/iter; left time: 699.3835s\n",
      "\titers: 600, epoch: 89 | loss: 0.1348187\n",
      "\tspeed: 0.0811s/iter; left time: 694.5205s\n",
      "\titers: 700, epoch: 89 | loss: 0.0929269\n",
      "\tspeed: 0.0795s/iter; left time: 673.5120s\n",
      "Epoch: 89 cost time: 60.16788959503174\n",
      "Epoch: 89, Steps: 764 | Train Loss: 0.1020886 Vali Loss: 0.1773832 Test Loss: 0.1039052\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Updating learning rate to 3.2311742677852644e-30\n",
      "\titers: 100, epoch: 90 | loss: 0.0859524\n",
      "\tspeed: 0.2092s/iter; left time: 1737.4062s\n",
      "\titers: 200, epoch: 90 | loss: 0.1180629\n",
      "\tspeed: 0.0786s/iter; left time: 645.1893s\n",
      "\titers: 300, epoch: 90 | loss: 0.1364252\n",
      "\tspeed: 0.0782s/iter; left time: 633.5382s\n",
      "\titers: 400, epoch: 90 | loss: 0.0763900\n",
      "\tspeed: 0.0784s/iter; left time: 627.3704s\n",
      "\titers: 500, epoch: 90 | loss: 0.0988563\n",
      "\tspeed: 0.0794s/iter; left time: 627.7183s\n",
      "\titers: 600, epoch: 90 | loss: 0.1012992\n",
      "\tspeed: 0.0789s/iter; left time: 615.8463s\n",
      "\titers: 700, epoch: 90 | loss: 0.1101373\n",
      "\tspeed: 0.0801s/iter; left time: 617.1741s\n",
      "Epoch: 90 cost time: 59.67092251777649\n",
      "Epoch: 90, Steps: 764 | Train Loss: 0.1020821 Vali Loss: 0.1788366 Test Loss: 0.1045243\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Updating learning rate to 1.6155871338926322e-30\n",
      "\titers: 100, epoch: 91 | loss: 0.0828224\n",
      "\tspeed: 0.2128s/iter; left time: 1604.5633s\n",
      "\titers: 200, epoch: 91 | loss: 0.0762683\n",
      "\tspeed: 0.0783s/iter; left time: 582.5937s\n",
      "\titers: 300, epoch: 91 | loss: 0.0888015\n",
      "\tspeed: 0.0785s/iter; left time: 576.2060s\n",
      "\titers: 400, epoch: 91 | loss: 0.1201894\n",
      "\tspeed: 0.0789s/iter; left time: 571.1435s\n",
      "\titers: 500, epoch: 91 | loss: 0.1330724\n",
      "\tspeed: 0.0782s/iter; left time: 558.0805s\n",
      "\titers: 600, epoch: 91 | loss: 0.1306803\n",
      "\tspeed: 0.0805s/iter; left time: 566.8121s\n",
      "\titers: 700, epoch: 91 | loss: 0.0854246\n",
      "\tspeed: 0.0792s/iter; left time: 550.0635s\n",
      "Epoch: 91 cost time: 60.105841875076294\n",
      "Epoch: 91, Steps: 764 | Train Loss: 0.1023593 Vali Loss: 0.1782656 Test Loss: 0.1032461\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Updating learning rate to 8.077935669463161e-31\n",
      "\titers: 100, epoch: 92 | loss: 0.1052879\n",
      "\tspeed: 0.2136s/iter; left time: 1447.6078s\n",
      "\titers: 200, epoch: 92 | loss: 0.0712274\n",
      "\tspeed: 0.0795s/iter; left time: 530.8892s\n",
      "\titers: 300, epoch: 92 | loss: 0.1270939\n",
      "\tspeed: 0.0781s/iter; left time: 513.8502s\n",
      "\titers: 400, epoch: 92 | loss: 0.0783173\n",
      "\tspeed: 0.0784s/iter; left time: 507.9788s\n",
      "\titers: 500, epoch: 92 | loss: 0.0798335\n",
      "\tspeed: 0.0785s/iter; left time: 500.5434s\n",
      "\titers: 600, epoch: 92 | loss: 0.0933270\n",
      "\tspeed: 0.0800s/iter; left time: 502.1530s\n",
      "\titers: 700, epoch: 92 | loss: 0.0972711\n",
      "\tspeed: 0.0796s/iter; left time: 491.4020s\n",
      "Epoch: 92 cost time: 60.21795678138733\n",
      "Epoch: 92, Steps: 764 | Train Loss: 0.1020404 Vali Loss: 0.1785259 Test Loss: 0.1034086\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Updating learning rate to 4.0389678347315805e-31\n",
      "\titers: 100, epoch: 93 | loss: 0.0828797\n",
      "\tspeed: 0.2117s/iter; left time: 1272.9669s\n",
      "\titers: 200, epoch: 93 | loss: 0.1311629\n",
      "\tspeed: 0.0795s/iter; left time: 470.0718s\n",
      "\titers: 300, epoch: 93 | loss: 0.0973252\n",
      "\tspeed: 0.0793s/iter; left time: 461.2265s\n",
      "\titers: 400, epoch: 93 | loss: 0.0673005\n",
      "\tspeed: 0.0805s/iter; left time: 459.7692s\n",
      "\titers: 500, epoch: 93 | loss: 0.1011205\n",
      "\tspeed: 0.0815s/iter; left time: 457.2895s\n",
      "\titers: 600, epoch: 93 | loss: 0.1240434\n",
      "\tspeed: 0.0803s/iter; left time: 442.9220s\n",
      "\titers: 700, epoch: 93 | loss: 0.1310788\n",
      "\tspeed: 0.0798s/iter; left time: 432.0075s\n",
      "Epoch: 93 cost time: 60.80596470832825\n",
      "Epoch: 93, Steps: 764 | Train Loss: 0.1019096 Vali Loss: 0.1772502 Test Loss: 0.1030775\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Updating learning rate to 2.0194839173657903e-31\n",
      "\titers: 100, epoch: 94 | loss: 0.1314521\n",
      "\tspeed: 0.2121s/iter; left time: 1113.1092s\n",
      "\titers: 200, epoch: 94 | loss: 0.1136785\n",
      "\tspeed: 0.0797s/iter; left time: 410.4454s\n",
      "\titers: 300, epoch: 94 | loss: 0.0881878\n",
      "\tspeed: 0.0786s/iter; left time: 397.0656s\n",
      "\titers: 400, epoch: 94 | loss: 0.0925845\n",
      "\tspeed: 0.0794s/iter; left time: 392.8529s\n",
      "\titers: 500, epoch: 94 | loss: 0.0911287\n",
      "\tspeed: 0.0800s/iter; left time: 387.8355s\n",
      "\titers: 600, epoch: 94 | loss: 0.1578123\n",
      "\tspeed: 0.0797s/iter; left time: 378.5085s\n",
      "\titers: 700, epoch: 94 | loss: 0.1316164\n",
      "\tspeed: 0.0796s/iter; left time: 370.2108s\n",
      "Epoch: 94 cost time: 60.515687704086304\n",
      "Epoch: 94, Steps: 764 | Train Loss: 0.1021601 Vali Loss: 0.1795195 Test Loss: 0.1028291\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Updating learning rate to 1.0097419586828951e-31\n",
      "\titers: 100, epoch: 95 | loss: 0.1314769\n",
      "\tspeed: 0.2125s/iter; left time: 952.9003s\n",
      "\titers: 200, epoch: 95 | loss: 0.0912714\n",
      "\tspeed: 0.0796s/iter; left time: 349.1282s\n",
      "\titers: 300, epoch: 95 | loss: 0.1026121\n",
      "\tspeed: 0.0807s/iter; left time: 345.6970s\n",
      "\titers: 400, epoch: 95 | loss: 0.0913465\n",
      "\tspeed: 0.0810s/iter; left time: 339.0412s\n",
      "\titers: 500, epoch: 95 | loss: 0.0682448\n",
      "\tspeed: 0.0795s/iter; left time: 324.5860s\n",
      "\titers: 600, epoch: 95 | loss: 0.0998566\n",
      "\tspeed: 0.0791s/iter; left time: 315.0585s\n",
      "\titers: 700, epoch: 95 | loss: 0.0787097\n",
      "\tspeed: 0.0795s/iter; left time: 308.8112s\n",
      "Epoch: 95 cost time: 60.50778269767761\n",
      "Epoch: 95, Steps: 764 | Train Loss: 0.1020841 Vali Loss: 0.1781962 Test Loss: 0.1039021\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Updating learning rate to 5.048709793414476e-32\n",
      "\titers: 100, epoch: 96 | loss: 0.0952979\n",
      "\tspeed: 0.2112s/iter; left time: 785.7269s\n",
      "\titers: 200, epoch: 96 | loss: 0.1272447\n",
      "\tspeed: 0.0793s/iter; left time: 287.2239s\n",
      "\titers: 300, epoch: 96 | loss: 0.1018355\n",
      "\tspeed: 0.0795s/iter; left time: 279.7772s\n",
      "\titers: 400, epoch: 96 | loss: 0.1263987\n",
      "\tspeed: 0.0797s/iter; left time: 272.6010s\n",
      "\titers: 500, epoch: 96 | loss: 0.0827369\n",
      "\tspeed: 0.0805s/iter; left time: 267.4240s\n",
      "\titers: 600, epoch: 96 | loss: 0.1367142\n",
      "\tspeed: 0.0789s/iter; left time: 254.2073s\n",
      "\titers: 700, epoch: 96 | loss: 0.1235133\n",
      "\tspeed: 0.0777s/iter; left time: 242.4416s\n",
      "Epoch: 96 cost time: 60.0967161655426\n",
      "Epoch: 96, Steps: 764 | Train Loss: 0.1021577 Vali Loss: 0.1797338 Test Loss: 0.1031268\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Updating learning rate to 2.524354896707238e-32\n",
      "\titers: 100, epoch: 97 | loss: 0.0929605\n",
      "\tspeed: 0.1732s/iter; left time: 512.1006s\n",
      "\titers: 200, epoch: 97 | loss: 0.1300679\n",
      "\tspeed: 0.0451s/iter; left time: 128.7806s\n",
      "\titers: 300, epoch: 97 | loss: 0.1181924\n",
      "\tspeed: 0.0456s/iter; left time: 125.7645s\n",
      "\titers: 400, epoch: 97 | loss: 0.1038760\n",
      "\tspeed: 0.0445s/iter; left time: 118.3291s\n",
      "\titers: 500, epoch: 97 | loss: 0.0989400\n",
      "\tspeed: 0.0450s/iter; left time: 115.1753s\n",
      "\titers: 600, epoch: 97 | loss: 0.1256611\n",
      "\tspeed: 0.0471s/iter; left time: 115.7585s\n",
      "\titers: 700, epoch: 97 | loss: 0.0928383\n",
      "\tspeed: 0.0455s/iter; left time: 107.2618s\n",
      "Epoch: 97 cost time: 34.96051645278931\n",
      "Epoch: 97, Steps: 764 | Train Loss: 0.1021066 Vali Loss: 0.1797877 Test Loss: 0.1043254\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Updating learning rate to 1.262177448353619e-32\n",
      "\titers: 100, epoch: 98 | loss: 0.0838296\n",
      "\tspeed: 0.1206s/iter; left time: 264.4455s\n",
      "\titers: 200, epoch: 98 | loss: 0.1143735\n",
      "\tspeed: 0.0461s/iter; left time: 96.5559s\n",
      "\titers: 300, epoch: 98 | loss: 0.1085821\n",
      "\tspeed: 0.0440s/iter; left time: 87.7344s\n",
      "\titers: 400, epoch: 98 | loss: 0.1291382\n",
      "\tspeed: 0.0434s/iter; left time: 82.2429s\n",
      "\titers: 500, epoch: 98 | loss: 0.1333422\n",
      "\tspeed: 0.0476s/iter; left time: 85.3046s\n",
      "\titers: 600, epoch: 98 | loss: 0.0960378\n",
      "\tspeed: 0.0435s/iter; left time: 73.6395s\n",
      "\titers: 700, epoch: 98 | loss: 0.1122958\n",
      "\tspeed: 0.0440s/iter; left time: 70.0306s\n",
      "Epoch: 98 cost time: 34.10540533065796\n",
      "Epoch: 98, Steps: 764 | Train Loss: 0.1018366 Vali Loss: 0.1776631 Test Loss: 0.1030247\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Updating learning rate to 6.310887241768095e-33\n",
      "\titers: 100, epoch: 99 | loss: 0.1033603\n",
      "\tspeed: 0.1198s/iter; left time: 171.1723s\n",
      "\titers: 200, epoch: 99 | loss: 0.1314797\n",
      "\tspeed: 0.0457s/iter; left time: 60.7323s\n",
      "\titers: 300, epoch: 99 | loss: 0.1394057\n",
      "\tspeed: 0.0452s/iter; left time: 55.5207s\n",
      "\titers: 400, epoch: 99 | loss: 0.0978522\n",
      "\tspeed: 0.0456s/iter; left time: 51.4425s\n",
      "\titers: 500, epoch: 99 | loss: 0.0953556\n",
      "\tspeed: 0.0450s/iter; left time: 46.3523s\n",
      "\titers: 600, epoch: 99 | loss: 0.1165339\n",
      "\tspeed: 0.0460s/iter; left time: 42.7585s\n",
      "\titers: 700, epoch: 99 | loss: 0.0963709\n",
      "\tspeed: 0.0454s/iter; left time: 37.6625s\n",
      "Epoch: 99 cost time: 34.80155324935913\n",
      "Epoch: 99, Steps: 764 | Train Loss: 0.1021830 Vali Loss: 0.1784641 Test Loss: 0.1040372\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Updating learning rate to 3.155443620884047e-33\n",
      "\titers: 100, epoch: 100 | loss: 0.1110248\n",
      "\tspeed: 0.1233s/iter; left time: 82.0134s\n",
      "\titers: 200, epoch: 100 | loss: 0.0913808\n",
      "\tspeed: 0.0443s/iter; left time: 25.0389s\n",
      "\titers: 300, epoch: 100 | loss: 0.0854594\n",
      "\tspeed: 0.0441s/iter; left time: 20.5178s\n",
      "\titers: 400, epoch: 100 | loss: 0.1235109\n",
      "\tspeed: 0.0446s/iter; left time: 16.2902s\n",
      "\titers: 500, epoch: 100 | loss: 0.0749040\n",
      "\tspeed: 0.0468s/iter; left time: 12.3932s\n",
      "\titers: 600, epoch: 100 | loss: 0.0916496\n",
      "\tspeed: 0.0457s/iter; left time: 7.5382s\n",
      "\titers: 700, epoch: 100 | loss: 0.0946029\n",
      "\tspeed: 0.0466s/iter; left time: 3.0286s\n",
      "Epoch: 100 cost time: 34.93826675415039\n",
      "Epoch: 100, Steps: 764 | Train Loss: 0.1020181 Vali Loss: 0.1787511 Test Loss: 0.1028742\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Updating learning rate to 1.5777218104420236e-33\n",
      "训练用时：6806.753595s\n",
      ">>>>>>>testing : informer_WTH_ftMS_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc3_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6989\n",
      "test shape: (218, 32, 24, 10) (218, 32, 24, 1)\n",
      "test shape: (6976, 24, 10) (6976, 24, 1)\n",
      "mse:0.10409028828144073, mae:0.2454308420419693, rmse:0.32263025641441345, mape:2.811321496963501, mspe:776.099853515625\n",
      "测试用时：3.298849s\n",
      "程序结束时间: 2024-05-17 14:11:05\n",
      "用时：6843.899485s\n"
     ]
    }
   ],
   "source": [
    "gpu_tracker = MemTracker()   \n",
    "gpu_tracker.track()\n",
    "\n",
    "t0 = time.time()\n",
    "print('程序开始时间:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
    "for ii in range(args.itr):\n",
    "    \n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    t0_train = time.time()\n",
    "    exp.train(setting)\n",
    "    t1_train = time.time()\n",
    "    print(\"训练用时：%.6fs\" % (t1_train - t0_train))\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    t0_test = time.time()\n",
    "    exp.test(setting)\n",
    "    t1_test = time.time()\n",
    "    print(\"测试用时：%.6fs\" % (t1_test - t0_test))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "t1 = time.time()\n",
    "print('程序结束时间:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
    "print(\"用时：%.6fs\" % (t1 - t0))\n",
    "\n",
    "gpu_tracker.track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDHF-HerAE3u"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s0YI1zx6ACiz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set saved model path\n",
    "\n",
    "setting = 'informer_custom_ftS_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc3_ebtimeF_dtTrue_mxTrue_exp_4'\n",
    "# path = os.path.join(args.checkpoints,setting,'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTkluNNcyMJt",
    "outputId": "780767fe-6321-4081-e827-6701daeb375b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "pred 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Informer:\n\tsize mismatch for enc_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([512, 1, 3]) from checkpoint, the shape in current model is torch.Size([512, 12, 3]).\n\tsize mismatch for dec_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([512, 1, 3]) from checkpoint, the shape in current model is torch.Size([512, 12, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you already have a trained model, you can set the arguments and model path, then initialize a Experiment and use it to predict\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Prediction is a sequence which is adjacent to the last date of the data, and does not exist in the data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# If you want to get more information about prediction, you can refer to code `exp/exp_informer.py function predict()` and `data/data_loader.py class Dataset_Pred`\u001b[39;00m\n\u001b[1;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/why/py_chaotic/Chaotic Deep Learning Model for time series prediction/informer_chaos/exp/exp_informer.py:260\u001b[0m, in \u001b[0;36mExp_Informer.predict\u001b[0;34m(self, setting, load)\u001b[0m\n\u001b[1;32m    258\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcheckpoints, setting)\n\u001b[1;32m    259\u001b[0m     best_model_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    264\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Informer:\n\tsize mismatch for enc_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([512, 1, 3]) from checkpoint, the shape in current model is torch.Size([512, 12, 3]).\n\tsize mismatch for dec_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([512, 1, 3]) from checkpoint, the shape in current model is torch.Size([512, 12, 3])."
     ]
    }
   ],
   "source": [
    "# If you already have a trained model, you can set the arguments and model path, then initialize a Experiment and use it to predict\n",
    "# Prediction is a sequence which is adjacent to the last date of the data, and does not exist in the data\n",
    "# If you want to get more information about prediction, you can refer to code `exp/exp_informer.py function predict()` and `data/data_loader.py class Dataset_Pred`\n",
    "\n",
    "exp = Exp(args)\n",
    "\n",
    "exp.predict(setting, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBCPbjGuzAZb",
    "outputId": "945dc447-88e8-4b08-b7e5-f0a0b486d138"
   },
   "outputs": [],
   "source": [
    "# the prediction will be saved in ./results/{setting}/real_prediction.npy\n",
    "import numpy as np\n",
    "\n",
    "prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yFuVkTV30_j"
   },
   "source": [
    "### More details about Prediction - prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv9AR_Aw030r"
   },
   "outputs": [],
   "source": [
    "# here is the detailed code of function predict\n",
    "\n",
    "def predict(exp, setting, load=False):\n",
    "    pred_data, pred_loader = exp._get_data(flag='pred')\n",
    "        \n",
    "    if load:\n",
    "        path = os.path.join(exp.args.checkpoints, setting)\n",
    "        best_model_path = path+'/'+'checkpoint.pth'\n",
    "        exp.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    exp.model.eval()\n",
    "        \n",
    "    preds = []\n",
    "        \n",
    "    for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(pred_loader):\n",
    "        batch_x = batch_x.float().to(exp.device)\n",
    "        batch_y = batch_y.float()\n",
    "        batch_x_mark = batch_x_mark.float().to(exp.device)\n",
    "        batch_y_mark = batch_y_mark.float().to(exp.device)\n",
    "\n",
    "        # decoder input\n",
    "        if exp.args.padding==0:\n",
    "            dec_inp = torch.zeros([batch_y.shape[0], exp.args.pred_len, batch_y.shape[-1]]).float()\n",
    "        elif exp.args.padding==1:\n",
    "            dec_inp = torch.ones([batch_y.shape[0], exp.args.pred_len, batch_y.shape[-1]]).float()\n",
    "        else:\n",
    "            dec_inp = torch.zeros([batch_y.shape[0], exp.args.pred_len, batch_y.shape[-1]]).float()\n",
    "        dec_inp = torch.cat([batch_y[:,:exp.args.label_len,:], dec_inp], dim=1).float().to(exp.device)\n",
    "        # encoder - decoder\n",
    "        if exp.args.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if exp.args.output_attention:\n",
    "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        else:\n",
    "            if exp.args.output_attention:\n",
    "                outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "            else:\n",
    "                outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        f_dim = -1 if exp.args.features=='MS' else 0\n",
    "        batch_y = batch_y[:,-exp.args.pred_len:,f_dim:].to(exp.device)\n",
    "        \n",
    "        pred = outputs.detach().cpu().numpy()#.squeeze()\n",
    "        \n",
    "        preds.append(pred)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "    \n",
    "    # result save\n",
    "    folder_path = './results/' + setting +'/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    np.save(folder_path+'real_prediction.npy', preds)\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVLWZL2a1pwB",
    "outputId": "421e9ae1-f024-42b6-c8cb-ed1d38c864cd"
   },
   "outputs": [],
   "source": [
    "# you can also use this prediction function to get result\n",
    "prediction = predict(exp, setting, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "NwtZmQC71uc8",
    "outputId": "eec9d116-f122-42d9-8e02-c893ff764db0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prediction[0,:,-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnePVyrW4I14"
   },
   "source": [
    "### More details about Prediction - prediction dataset\n",
    "\n",
    "You can give a `root_path` and `data_path` of the data you want to forecast, and set `seq_len`, `label_len`, `pred_len` and other arguments as other Dataset. The difference is that you can set a more detailed freq such as `15min` or `3h` to generate the timestamp of prediction series.\n",
    "\n",
    "`Dataset_Pred` only has one sample (including `encoder_input: [1, seq_len, dim]`, `decoder_token: [1, label_len, dim]`, `encoder_input_timestamp: [1, seq_len, date_dim]`, `decoder_input_timstamp: [1, label_len+pred_len, date_dim]`). It will intercept the last sequence of the given data (seq_len data) to forecast the unseen future sequence (pred_len data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpXhNGp34Hf4"
   },
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Pred\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Rpd1q74T8N"
   },
   "outputs": [],
   "source": [
    "Data = Dataset_Pred\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'pred'; shuffle_flag = False; drop_last = False; batch_size = 1\n",
    "\n",
    "freq = args.detail_freq\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    target=args.target,\n",
    "    timeenc=timeenc,\n",
    "    freq=freq\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42C84BfY6UPV",
    "outputId": "f5ccc428-db92-4708-e104-f5d29aa5adf9"
   },
   "outputs": [],
   "source": [
    "len(data_set), len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNhEP_7sAgqC"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMRk8VkQ2Iko",
    "outputId": "bbf3cd10-7294-472d-e330-21e00f20963a"
   },
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "preds = np.load('./results/'+setting+'/pred.npy')\n",
    "trues = np.load('./results/'+setting+'/true.npy')\n",
    "\n",
    "# [samples, pred_len, dimensions]\n",
    "preds.shape, trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEGhDOmxAeAb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "kyPuOPGAAjl3",
    "outputId": "8554f6f8-c13a-43e1-b04b-5f27823445d0"
   },
   "outputs": [],
   "source": [
    "# draw OT prediction\n",
    "plt.figure()\n",
    "plt.plot(trues[0,:,-1], label='GroundTruth')\n",
    "plt.plot(preds[0,:,-1], label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "43MIgWfpMYIB",
    "outputId": "327f64b7-363c-44f9-c7c8-1f654911068c"
   },
   "outputs": [],
   "source": [
    "# draw HUFL prediction\n",
    "plt.figure()\n",
    "plt.plot(trues[0,:,0], label='GroundTruth')\n",
    "plt.plot(preds[0,:,0], label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKmqhCfmt0xd"
   },
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_ETT_hour\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Data = Dataset_ETT_hour\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    freq=args.freq\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iflTTl0quCoK",
    "outputId": "3708fc91-517e-4c83-e133-059381bde271"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "args.output_attention = True\n",
    "\n",
    "exp = Exp(args)\n",
    "\n",
    "model = exp.model\n",
    "\n",
    "setting = 'informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0'\n",
    "path = os.path.join(args.checkpoints,setting,'checkpoint.pth')\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDdzqm9HAk2C"
   },
   "outputs": [],
   "source": [
    "# attention visualization\n",
    "idx = 0\n",
    "for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(data_loader):\n",
    "    if i!=idx:\n",
    "        continue\n",
    "    batch_x = batch_x.float().to(exp.device)\n",
    "    batch_y = batch_y.float()\n",
    "\n",
    "    batch_x_mark = batch_x_mark.float().to(exp.device)\n",
    "    batch_y_mark = batch_y_mark.float().to(exp.device)\n",
    "    \n",
    "    dec_inp = torch.zeros_like(batch_y[:,-args.pred_len:,:]).float()\n",
    "    dec_inp = torch.cat([batch_y[:,:args.label_len,:], dec_inp], dim=1).float().to(exp.device)\n",
    "    \n",
    "    outputs,attn = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWef23vWAmUz",
    "outputId": "021eca83-e12f-402c-c87e-4fffa643d2f1"
   },
   "outputs": [],
   "source": [
    "attn[0].shape, attn[1].shape #, attn[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iZDH1fZgAnrl",
    "outputId": "991cae95-04a2-402d-f179-777e962f46fe"
   },
   "outputs": [],
   "source": [
    "layer = 0\n",
    "distil = 'Distil' if args.distil else 'NoDistil'\n",
    "for h in range(0,8):\n",
    "    plt.figure(figsize=[10,8])\n",
    "    plt.title('Informer, {}, attn:{} layer:{} head:{}'.format(distil, args.attn, layer, h))\n",
    "    A = attn[layer][0,h].detach().cpu().numpy()\n",
    "    ax = sns.heatmap(A, vmin=0, vmax=A.max()+0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DQFGYE3KAozo",
    "outputId": "10489845-4e23-4b55-8509-edf50d77394d"
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "distil = 'Distil' if args.distil else 'NoDistil'\n",
    "for h in range(0,8):\n",
    "    plt.figure(figsize=[10,8])\n",
    "    plt.title('Informer, {}, attn:{} layer:{} head:{}'.format(distil, args.attn, layer, h))\n",
    "    A = attn[layer][0,h].detach().cpu().numpy()\n",
    "    ax = sns.heatmap(A, vmin=0, vmax=A.max()+0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew9vekI9Mw8f"
   },
   "source": [
    "## Custom Data\n",
    "\n",
    "Custom data (xxx.csv) has to include at least 2 features: `date`(format: `YYYY-MM-DD hh:mm:ss`) and `target feature`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqQBJWHeMzP-"
   },
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bFrfuw6Oxpi"
   },
   "outputs": [],
   "source": [
    "# custom data: xxx.csv\n",
    "# data features: ['date', ...(other features), target feature]\n",
    "\n",
    "# we take ETTh2 as an example\n",
    "args.root_path = './ETDataset/ETT-small/'\n",
    "args.data_path = 'ETTh2.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(args.root_path, args.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0--9JC0eO_WT",
    "outputId": "c8b9c2a3-0400-44d4-8a4e-85e42194ea59"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmvRipRbPAbP"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We set 'HULL' as target instead of 'OT'\n",
    "\n",
    "The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "'''\n",
    "\n",
    "args.target = 'HULL'\n",
    "args.freq = 'h'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkNDT2jMPCUf"
   },
   "outputs": [],
   "source": [
    "batch_x,batch_y,batch_x_mark,batch_y_mark = data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUcvSLlkSFTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
